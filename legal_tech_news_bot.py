#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³•å¾‹ç§‘æŠ€æ–°é—»æ±‡æ€»Bot
åŠŸèƒ½ï¼šæ¯å¤©è‡ªåŠ¨æŠ“å–Legal Techæ–°é—»ï¼Œç”¨AIæ•´ç†æˆä¸­æ–‡Newsletterï¼Œé€šè¿‡é£ä¹¦æ¨é€
ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025
"""

import os        # ç”¨äºè¯»å–ç¯å¢ƒå˜é‡
import json      # ç”¨äºå¤„ç†JSONæ•°æ®
import time      # ç”¨äºæ—¶é—´å¤„ç†å’Œç­‰å¾…
import schedule  # ç”¨äºå®šæ—¶ä»»åŠ¡
import logging   # ç”¨äºæ—¥å¿—è®°å½•
from datetime import datetime
from typing import List, Dict

import requests      # ç”¨äºHTTPè¯·æ±‚
from dotenv import load_dotenv  # ç”¨äºåŠ è½½.envé…ç½®æ–‡ä»¶
from deep_translator import GoogleTranslator  # ç”¨äºå…è´¹ç¿»è¯‘
import feedparser      # ç”¨äºRSSè§£æ
import re              # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ¸…ç†HTML
import string          # ç”¨äºå­—ç¬¦ä¸²å¤„ç†
from difflib import SequenceMatcher  # ç”¨äºè®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦

# ====================== é…ç½®æ—¥å¿—ç³»ç»Ÿ ======================
# æ—¥å¿—ä¼šè¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°ï¼Œæ–¹ä¾¿è°ƒè¯•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('news_bot.log', encoding='utf-8'),  # è¾“å‡ºåˆ°æ–‡ä»¶
        logging.StreamHandler()  # è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)
logger = logging.getLogger(__name__)


# ====================== é…ç½®ç®¡ç† ======================
class Config:
    """é…ç½®ç±»ï¼šç»Ÿä¸€ç®¡ç†æ‰€æœ‰é…ç½®é¡¹"""

    def __init__(self):
        """åŠ è½½.envæ–‡ä»¶ä¸­çš„é…ç½®"""
        # åŠ è½½.envæ–‡ä»¶
        load_dotenv()

        # ========== å¿…éœ€é…ç½® ==========
        # NewsAPIé…ç½®
        self.news_api_key = os.getenv('NEWS_API_KEY')
        self.news_api_url = 'https://newsapi.org/v2/everything'
        # NewsAPIå…è´¹ç‰ˆé™åˆ¶ï¼šæ¯å¤©æœ€å¤š100æ¬¡è¯·æ±‚
        self.news_api_daily_limit = 100  # å…è´¹ç‰ˆæ¯æ—¥é™åˆ¶
        self.news_api_request_count = 0  # å½“æ—¥å·²ä½¿ç”¨æ¬¡æ•°

        # Claude APIé…ç½®
        self.claude_api_key = os.getenv('CLAUDE_API_KEY')
        self.claude_api_url = 'https://api.anthropic.com/v1/messages'
        # Claude Haikuä»·æ ¼ï¼šçº¦$0.25/ç™¾ä¸‡è¾“å…¥tokenï¼Œ$1.25/ç™¾ä¸‡è¾“å‡ºtoken
        self.claude_max_tokens = 2000  # æ¯æ¬¡è¯·æ±‚æœ€å¤§tokenæ•°

        # é£ä¹¦æœºå™¨äººé…ç½®
        self.feishu_webhook = os.getenv('FEISHU_WEBHOOK_URL')

        # ========== å¯é€‰é…ç½® ==========
        # æ–°é—»æœç´¢å…³é”®è¯ï¼ˆè‹±æ–‡ï¼‰
        self.search_keywords = os.getenv('SEARCH_KEYWORDS',
                                       'legal tech OR legal technology OR lawtech OR legal AI')

        # ä¸­æ–‡æ–°é—»æœç´¢å…³é”®è¯ï¼ˆç”¨äºè·å–å›½å†…æ–°é—»ï¼‰
        self.search_keywords_cn = os.getenv('SEARCH_KEYWORDS_CN',
                                           'æ³•å¾‹ç§‘æŠ€ OR æ³•å¾‹AI OR æ³•åŠ¡AI OR æ™ºèƒ½æ³•åŠ¡ OR æ³•å¾‹å¤§æ¨¡å‹ OR æ³•å¾‹äººå·¥æ™ºèƒ½')

        # è¿è¡Œæ¨¡å¼ï¼štestï¼ˆæµ‹è¯•æ¨¡å¼ï¼Œç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼‰æˆ– productionï¼ˆç”Ÿäº§æ¨¡å¼ï¼Œå®šæ—¶è¿è¡Œï¼‰
        self.run_mode = os.getenv('RUN_MODE', 'production').lower()

        # æ˜¯å¦å¯ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼ˆå½“Claude APIå¤±è´¥æ—¶ï¼‰
        self.enable_fallback = os.getenv('ENABLE_FALLBACK', 'true').lower() == 'true'

        # æ–°é—»æ•°é‡é™åˆ¶
        self.max_articles = int(os.getenv('MAX_ARTICLES', '15'))

        # ========== RSSæ–°é—»æºé…ç½® ==========

        # ---------- æ³•å¾‹ç§‘æŠ€ä¸“ä¸šæº ----------
        # 1. Google News RSSï¼ˆå¤šè¯­è¨€ï¼ŒåŸºäºå…³é”®è¯æœç´¢ï¼‰
        self.rss_google_news = 'https://news.google.com/rss/search?q=legal+tech+OR+legal+AI+OR+æ³•å¾‹ç§‘æŠ€&hl=zh-CN&gl=CN&ceid=CN:zh-Hans'

        # 2. JDSupra Legal Tech RSSï¼ˆè‹±æ–‡ï¼Œç§‘æŠ€ç›¸å…³æ³•å¾‹æ–°é—»ï¼‰
        self.rss_jdsupra = 'https://www.jdsupra.com/resources/syndication/docsRSSfeed.aspx?ftype=ScienceComputersTechnology&premium=1'

        # 3. Artificial Lawyer RSSï¼ˆè‹±æ–‡ï¼Œæ³•å¾‹AIä¸“ä¸šåšå®¢ï¼‰
        self.rss_artificial_lawyer = 'https://www.artificiallawyer.com/feed/'

        # 4. Above the Law RSSï¼ˆè‹±æ–‡ï¼Œæ³•å¾‹æ–°é—»ï¼‰
        self.rss_above_the_law = 'https://abovethelaw.com/feed/'

        # 5. TechLaw RSSï¼ˆè‹±æ–‡ï¼Œæ³•å¾‹ç§‘æŠ€ï¼‰
        self.rss_techlaw = 'https://feeds.feedburner.com/techlaw'

        # ---------- å›½å†…ç§‘æŠ€åª’ä½“æºï¼ˆæ³•å¾‹+AIï¼‰ ----------
        # 6. 36æ°ª - äººå·¥æ™ºèƒ½é¢‘é“
        self.rss_36kr_ai = 'https://36kr.com/api/channel/1011/kr-column-list?per_page=20'

        # 7. è™å—…ç½‘ - ç§‘æŠ€é¢‘é“
        self.rss_huxiu = 'https://www.huxiu.com/rss/0.xml'

        # 8. é’›åª’ä½“ - ç§‘æŠ€é¢‘é“
        self.rss_ttm = 'https://www.tmtpost.com/rss/caijing.xml'

        # 9. InfoQ ä¸­æ–‡ - AIé¢‘é“
        self.rss_infoq = 'https://www.infoq.cn/feed'

        # 10. æå®¢å…¬å›­ - ç§‘æŠ€èµ„è®¯
        self.rss_geekpark = 'https://www.geekpark.net/rss'

        # ---------- AIä¸“ä¸šåª’ä½“æº ----------
        # 11. é‡å­ä½ - å›½å†…AIä¸“ä¸šåª’ä½“
        self.rss_qbitai = 'https://www.qbitai.com/feed'

        # 12. æœºå™¨ä¹‹å¿ƒ - AIä¸“ä¸šåª’ä½“
        self.rss_jiqizhixin = 'https://www.jiqizhixin.com/rss'

        # 13. æ–°æ™ºå…ƒ - AIèµ„è®¯
        self.rss_aiyuan = 'https://www.ai-yuan.com/feed'

        # ---------- å›½å¤–AIæƒå¨æº ----------
        # 14. TechCrunch AI
        self.rss_techcrunch_ai = 'https://techcrunch.com/category/artificial-intelligence/feed/'

        # 15. The Verge AI
        self.rss_verge_ai = 'https://www.theverge.com/ai-artificial-intelligence/rss/index.xml'

        # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        self._validate_config()
        self._log_api_limits()

    def _validate_config(self):
        """éªŒè¯æ‰€æœ‰å¿…éœ€çš„é…ç½®æ˜¯å¦å­˜åœ¨"""
        required_configs = {
            'NEWS_API_KEY': self.news_api_key,
            'FEISHU_WEBHOOK_URL': self.feishu_webhook
        }

        # Claude APIå¯†é’¥æ˜¯å¯é€‰çš„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç®€å•æ ¼å¼
        optional_configs = {
            'CLAUDE_API_KEY': self.claude_api_key
        }

        missing_configs = [name for name, value in required_configs.items() if not value]

        if missing_configs:
            raise ValueError(
                f"ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: {', '.join(missing_configs)}\n"
                f"è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®è¿™äº›é¡¹"
            )

        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        logger.info(f"ğŸ“Š è¿è¡Œæ¨¡å¼: {self.run_mode}")

        # æç¤ºClaude APIçŠ¶æ€
        if not self.claude_api_key:
            logger.info("ğŸ’¡ æœªé…ç½®Claude APIï¼Œå°†ä½¿ç”¨ç®€å•æ ¼å¼ï¼ˆè‹±æ–‡åŸæ–°é—»ï¼‰")
        else:
            logger.info("âœ… å·²é…ç½®Claude APIï¼Œå°†ç”Ÿæˆä¸­æ–‡Newsletter")

    def _log_api_limits(self):
        """è®°å½•APIé™åˆ¶ä¿¡æ¯"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ APIé™åˆ¶è¯´æ˜")
        logger.info("=" * 60)
        logger.info(f"NewsAPI: å…è´¹ç‰ˆæ¯å¤©æœ€å¤š {self.news_api_daily_limit} æ¬¡è¯·æ±‚")

        if self.claude_api_key:
            logger.info(f"Claude API: Haikuæ¨¡å‹ï¼Œçº¦$0.25/ç™¾ä¸‡è¾“å…¥token")
            logger.info(f"å½“å‰é…ç½®: æœ€å¤§{self.claude_max_tokens} tokens/æ¬¡")
        else:
            logger.info("Claude API: æœªé…ç½®ï¼ˆä½¿ç”¨å…è´¹ç®€å•æ ¼å¼ï¼‰")

        logger.info(f"å¤‡ç”¨æ–¹æ¡ˆ: {'âœ… å·²å¯ç”¨' if self.enable_fallback else 'âŒ å·²ç¦ç”¨'}")
        logger.info("=" * 60 + "\n")


# ====================== å¤šæºæ–°é—»è·å–æ¨¡å— ======================
class NewsFetcher:
    """æ–°é—»è·å–ç±»ï¼šä»å¤šä¸ªæ¥æºï¼ˆNewsAPI + RSSï¼‰è·å–æ³•å¾‹ç§‘æŠ€æ–°é—»"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–æ–°é—»è·å–å™¨
        :param config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.session = requests.Session()  # ä½¿ç”¨Sessionå¯ä»¥æé«˜HTTPè¯·æ±‚æ•ˆç‡

    def _fetch_from_rss(self, rss_url: str, source_name: str, max_items: int = 10) -> List[Dict]:
        """
        ä»RSSæºè·å–æ–°é—»
        :param rss_url: RSSé“¾æ¥
        :param source_name: æºåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        :param max_items: æœ€å¤§è·å–æ•°é‡
        :return: æ–°é—»åˆ—è¡¨
        """
        articles = []
        try:
            logger.info(f"ğŸ“¡ æ­£åœ¨è·å– {source_name} RSS...")
            feed = feedparser.parse(rss_url)

            if feed.bozo:
                logger.warning(f"âš ï¸ {source_name} RSSè§£æå¯èƒ½æœ‰è¯¯: {feed.bozo_exception}")

            if not feed.entries:
                logger.warning(f"âš ï¸ {source_name} RSSæ²¡æœ‰è¿”å›ä»»ä½•å†…å®¹")
                return articles

            # è§£æRSSæ¡ç›®
            for entry in feed.entries[:max_items]:
                # æå–å‘å¸ƒæ—¶é—´
                published_at = ''
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    try:
                        published_at = datetime(*entry.published_parsed[:6]).isoformat()
                    except:
                        pass
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    try:
                        published_at = datetime(*entry.updated_parsed[:6]).isoformat()
                    except:
                        pass

                # æ„å»ºæ ‡å‡†åŒ–çš„æ–°é—»æ ¼å¼
                # æ¸…ç†descriptionä¸­çš„HTMLæ ‡ç­¾
                description_raw = entry.get('description', '')
                title_raw = entry.get('title', 'æ— æ ‡é¢˜')

                # æ¸…ç†HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºæ ¼
                description_clean = re.sub(r'<[^>]+>', '', description_raw)
                description_clean = re.sub(r'\s+', ' ', description_clean).strip()

                # å¦‚æœdescriptionä¸ºç©ºæˆ–ä¸æ ‡é¢˜ç›¸åŒï¼Œå°è¯•ä»contentä¸­è·å–
                if not description_clean or description_clean.lower() == title_raw.lower():
                    if hasattr(entry, 'content') and entry.get('content'):
                        content_raw = entry.get('content', [{}])[0].get('value', '')
                        description_clean = re.sub(r'<[^>]+>', '', content_raw)
                        description_clean = re.sub(r'\s+', ' ', description_clean).strip()

                        # é™åˆ¶é•¿åº¦ï¼ˆå–å‰200ä¸ªå­—ç¬¦ï¼‰
                        if len(description_clean) > 200:
                            description_clean = description_clean[:200] + '...'

                    # å¦‚æœcontentä¹Ÿæ²¡æœ‰æˆ–è€…è¿˜æ˜¯å’Œæ ‡é¢˜ä¸€æ ·ï¼Œå°±ç•™ç©º
                    if not description_clean or description_clean.lower() == title_raw.lower():
                        description_clean = ''

                article = {
                    'title': title_raw,
                    'description': description_clean,
                    'url': entry.get('link', ''),
                    'source': {'name': source_name},
                    'publishedAt': published_at,
                    'content': entry.get('content', [{}])[0].get('value', '') if hasattr(entry, 'content') else ''
                }
                articles.append(article)

            logger.info(f"âœ… {source_name} RSSè·å– {len(articles)} æ¡")

        except Exception as e:
            logger.error(f"âŒ è·å– {source_name} RSSå¤±è´¥: {e}")

        return articles

    def fetch_legal_tech_news(self) -> List[Dict]:
        """
        ä»å¤šä¸ªæ¥æºï¼ˆ5ä¸ªRSS + NewsAPIï¼‰è·å–æ³•å¾‹ç§‘æŠ€æ–°é—»
        :return: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«æ ‡é¢˜ã€æè¿°ã€URLã€æ¥æºç­‰
        """
        logger.info("ğŸ” å¼€å§‹è·å–æ³•å¾‹ç§‘æŠ€æ–°é—»ï¼ˆå¤šæºæ¨¡å¼ï¼‰...")

        all_articles = []

        # ========== ç¬¬ä¸€æ­¥ï¼šä»5ä¸ªRSSæºè·å–æ–°é—»ï¼ˆå…è´¹ï¼Œæ— é™åˆ¶ï¼‰==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“¡ å¼€å§‹è·å–RSSæ–°é—»æºï¼ˆ5ä¸ªæºï¼‰...")
        logger.info("=" * 60)

        # 1. Google News RSS
        google_news_articles = self._fetch_from_rss(
            self.config.rss_google_news,
            'Google News',
            max_items=20
        )
        all_articles.extend(google_news_articles)

        # 2. JDSupra Legal Tech RSS
        jdsupra_articles = self._fetch_from_rss(
            self.config.rss_jdsupra,
            'JDSupra Legal Tech',
            max_items=20
        )
        all_articles.extend(jdsupra_articles)

        # 3. Artificial Lawyer RSS
        artificial_lawyer_articles = self._fetch_from_rss(
            self.config.rss_artificial_lawyer,
            'Artificial Lawyer',
            max_items=20
        )
        all_articles.extend(artificial_lawyer_articles)

        # 4. Above the Law RSS
        above_the_law_articles = self._fetch_from_rss(
            self.config.rss_above_the_law,
            'Above the Law',
            max_items=20
        )
        all_articles.extend(above_the_law_articles)

        # 5. TechLaw RSS
        techlaw_articles = self._fetch_from_rss(
            self.config.rss_techlaw,
            'TechLaw',
            max_items=10
        )
        all_articles.extend(techlaw_articles)

        logger.info(f"\nâœ… æ³•å¾‹ç§‘æŠ€RSSæºå…±è·å– {len(all_articles)} æ¡æ–°é—»")

        # ========== ç¬¬äºŒæ­¥ï¼šè·å–å›½å†…ç§‘æŠ€åª’ä½“æ–°é—» ==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“¡ å¼€å§‹è·å–å›½å†…ç§‘æŠ€åª’ä½“æ–°é—»ï¼ˆ5ä¸ªæºï¼‰...")
        logger.info("=" * 60)

        # 6. è™å—…ç½‘
        huxiu_articles = self._fetch_from_rss(
            self.config.rss_huxiu,
            'è™å—…ç½‘',
            max_items=15
        )
        all_articles.extend(huxiu_articles)

        # 7. é’›åª’ä½“
        ttm_articles = self._fetch_from_rss(
            self.config.rss_ttm,
            'é’›åª’ä½“',
            max_items=15
        )
        all_articles.extend(ttm_articles)

        # 8. InfoQ ä¸­æ–‡
        infoq_articles = self._fetch_from_rss(
            self.config.rss_infoq,
            'InfoQ',
            max_items=15
        )
        all_articles.extend(infoq_articles)

        # 9. æå®¢å…¬å›­
        geekpark_articles = self._fetch_from_rss(
            self.config.rss_geekpark,
            'æå®¢å…¬å›­',
            max_items=10
        )
        all_articles.extend(geekpark_articles)

        logger.info(f"\nâœ… å›½å†…ç§‘æŠ€åª’ä½“å…±è·å– {len(all_articles)} æ¡æ–°é—»")

        # ========== ç¬¬ä¸‰æ­¥ï¼šè·å–AIä¸“ä¸šåª’ä½“æ–°é—» ==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ¤– å¼€å§‹è·å–AIä¸“ä¸šåª’ä½“æ–°é—»ï¼ˆ5ä¸ªæºï¼‰...")
        logger.info("=" * 60)

        # 10. é‡å­ä½
        qbitai_articles = self._fetch_from_rss(
            self.config.rss_qbitai,
            'é‡å­ä½',
            max_items=20
        )
        all_articles.extend(qbitai_articles)

        # 11. æœºå™¨ä¹‹å¿ƒ
        jiqizhixin_articles = self._fetch_from_rss(
            self.config.rss_jiqizhixin,
            'æœºå™¨ä¹‹å¿ƒ',
            max_items=20
        )
        all_articles.extend(jiqizhixin_articles)

        # 12. æ–°æ™ºå…ƒ
        aiyuan_articles = self._fetch_from_rss(
            self.config.rss_aiyuan,
            'æ–°æ™ºå…ƒ',
            max_items=15
        )
        all_articles.extend(aiyuan_articles)

        # 13. TechCrunch AI
        techcrunch_ai_articles = self._fetch_from_rss(
            self.config.rss_techcrunch_ai,
            'TechCrunch AI',
            max_items=15
        )
        all_articles.extend(techcrunch_ai_articles)

        # 14. The Verge AI
        verge_ai_articles = self._fetch_from_rss(
            self.config.rss_verge_ai,
            'The Verge AI',
            max_items=10
        )
        all_articles.extend(verge_ai_articles)

        logger.info(f"\nâœ… AIä¸“ä¸šåª’ä½“å…±è·å– {len(all_articles)} æ¡æ–°é—»")

        # ========== ç¬¬å››æ­¥ï¼šä»NewsAPIè·å–æ–°é—»ï¼ˆè¡¥å……ï¼‰==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“° å¼€å§‹è·å–NewsAPIæ–°é—»...")
        logger.info("=" * 60)

        # æ£€æŸ¥APIé…é¢
        self.config.news_api_request_count += 1
        remaining_requests = self.config.news_api_daily_limit - self.config.news_api_request_count

        if remaining_requests <= 0:
            logger.warning(f"âš ï¸ NewsAPIé…é¢å·²ç”¨å®Œï¼Œä»…ä½¿ç”¨RSSæº")
        elif remaining_requests <= 10:
            logger.warning(f"âš ï¸ NewsAPIé…é¢å³å°†ç”¨å°½ï¼šå‰©ä½™ {remaining_requests}/{self.config.news_api_daily_limit} æ¬¡")

        try:
            # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆå‰3å¤©ï¼‰
            from datetime import timedelta
            from_date = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
            today = datetime.now().strftime('%Y-%m-%d')
            logger.info(f"ğŸ“… æœç´¢æ—¥æœŸèŒƒå›´: {from_date} è‡³ {today}ï¼ˆè¿‘3å¤©ï¼‰")

            # æ„å»ºæœç´¢å…³é”®è¯
            precise_keywords = 'legal tech OR legaltech OR legal AI OR law technology OR æ³•å¾‹ç§‘æŠ€ OR æ³•å¾‹AI'
            logger.info(f"ğŸ”‘ æœç´¢å…³é”®è¯: {precise_keywords}")

            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            params = {
                'q': precise_keywords,
                'sortBy': 'relevance',
                'from': from_date,
                'to': today,
                'pageSize': 20,  # è·å–20æ¡ç”¨äºè¡¥å……
                'apiKey': self.config.news_api_key
            }

            # å‘é€APIè¯·æ±‚
            response = self.session.get(self.config.news_api_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()

            if data.get('status') == 'ok':
                newsapi_articles = data.get('articles', [])
                all_articles.extend(newsapi_articles)
                logger.info(f"âœ… NewsAPIè·å– {len(newsapi_articles)} æ¡æ–°é—»")

        except Exception as e:
            logger.warning(f"âš ï¸ NewsAPIè·å–å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨RSSæº")

        # ========== ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½å»é‡ï¼ˆURL + æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ” å¼€å§‹æ™ºèƒ½å»é‡...")
        logger.info("=" * 60)

        def is_similar_title(title1, title2):
            """åˆ¤æ–­ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦ç›¸ä¼¼ï¼ˆé¿å…é‡å¤æ–°é—»ï¼‰"""
            if not title1 or not title2:
                return False

            # æ¸…ç†æ ‡é¢˜
            t1 = title1.lower().strip()
            t2 = title2.lower().strip()

            # å®Œå…¨ç›¸åŒ
            if t1 == t2:
                return True

            # ä¸€ä¸ªæ ‡é¢˜æ˜¯å¦ä¸€ä¸ªçš„å­é›†ï¼ˆè¶…è¿‡80%ç›¸ä¼¼åº¦ï¼‰
            if len(t1) > 0 and len(t2) > 0:
                if t1 in t2 or t2 in t1:
                    # æ£€æŸ¥é•¿åº¦ç›¸ä¼¼åº¦
                    ratio = min(len(t1), len(t2)) / max(len(t1), len(t2))
                    if ratio >= 0.8:  # 80%ä»¥ä¸Šç›¸ä¼¼
                        return True

            return False

        # ç¬¬ä¸€å±‚å»é‡ï¼šURLå»é‡
        seen_urls = set()
        unique_by_url = []
        for article in all_articles:
            url = article.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_by_url.append(article)

        logger.info(f"âœ… URLå»é‡åå‰©ä½™ {len(unique_by_url)} æ¡æ–°é—»")

        # ç¬¬äºŒå±‚å»é‡ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦å»é‡
        unique_articles = []
        seen_titles = []

        for article in unique_by_url:
            title = article.get('title', '')
            is_duplicate = False

            # æ£€æŸ¥æ˜¯å¦ä¸å·²è§è¿‡çš„æ ‡é¢˜ç›¸ä¼¼
            for seen_title in seen_titles:
                if is_similar_title(title, seen_title):
                    is_duplicate = True
                    logger.debug(f"ğŸ”„ å‘ç°ç›¸ä¼¼æ ‡é¢˜ï¼Œå·²å»é‡: {title[:50]}...")
                    break

            if not is_duplicate:
                seen_titles.append(title)
                unique_articles.append(article)

        removed_count = len(unique_by_url) - len(unique_articles)
        if removed_count > 0:
            logger.info(f"âœ… æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ï¼šç§»é™¤ {removed_count} æ¡é‡å¤æ–°é—»")

        logger.info(f"âœ… æœ€ç»ˆå»é‡åå‰©ä½™ {len(unique_articles)} æ¡æ–°é—»")

        # ========== ç¬¬äº”æ­¥ï¼šæ™ºèƒ½è¯„åˆ†æ’åºï¼ˆæ¥æºæƒé‡ + ç›¸å…³æ€§è¯„åˆ†ï¼‰+ æ–°é—»åˆ†ç±» ==========
        logger.info("ğŸ¯ å¼€å§‹æ™ºèƒ½è¯„åˆ†æ’åºå’Œåˆ†ç±»...")

        # å®šä¹‰æ¥æºæƒé‡
        source_weights = {
            # æ³•å¾‹ç§‘æŠ€ä¸“ä¸šæº
            'Artificial Lawyer': 3.0,
            'JDSupra Legal Tech': 3.0,
            'Above the Law': 2.0,
            'Law.com': 2.0,
            'LegalTechnology.News': 2.5,
            'TechLaw': 2.0,
            # AIä¸“ä¸šåª’ä½“ï¼ˆé«˜æƒé‡ï¼‰
            'é‡å­ä½': 3.0,
            'æœºå™¨ä¹‹å¿ƒ': 3.0,
            'æ–°æ™ºå…ƒ': 2.8,
            'TechCrunch AI': 2.5,
            'The Verge AI': 2.5,
            # å›½å†…ç§‘æŠ€åª’ä½“
            'è™å—…ç½‘': 1.8,
            'é’›åª’ä½“': 1.8,
            'InfoQ': 1.8,
            'æå®¢å…¬å›­': 1.5,
            '36æ°ª': 1.5,
            # é€šç”¨æ–°é—»
            'Google News': 1.5,
            'Business Insider': 1.0,
            'TechCrunch': 1.0,
            'Forbes': 1.0,
        }

        # ========== æ³•å¾‹ç§‘æŠ€å…³é”®è¯ ==========
        legal_tech_keywords = [
            'legal ai', 'legal artificial intelligence', 'æ³•å¾‹ai', 'æ³•å¾‹AI', 'æ³•å¾‹äººå·¥æ™ºèƒ½'
        ]

        legal_tech_secondary = [
            'legal tech', 'legal technology', 'legaltech', 'lawtech',
            'law tech', 'law technology', 'æ³•å¾‹ç§‘æŠ€',
            'legal automation', 'contract AI', 'e-discovery', 'document automation'
        ]

        # ========== AIé‡å¤§æ–°é—»å…³é”®è¯ï¼ˆå›½å†… + å›½å¤–ï¼‰==========
        # å›½å†…AIå…¬å¸å’Œäº§å“
        ai_major_domestic = [
            # å¤§æ¨¡å‹äº§å“
            'kimi', 'æœˆä¹‹æš—é¢', 'moonshot',
            'chatglm', 'æ™ºè°±ai', 'æ™ºè°±AI', 'zhipu',
            'æ–‡å¿ƒä¸€è¨€', 'ernie bot', 'ç™¾åº¦ai', 'paddlepaddle',
            'é€šä¹‰åƒé—®', 'qwen', 'é˜¿é‡Œai', 'é€šä¹‰',
            'æ··å…ƒ', 'hunyuan', 'è…¾è®¯ai',
            'æ˜Ÿç«', 'è®¯é£ai', 'iflytek',
            'è±†åŒ…', 'å­—èŠ‚ai', 'å­—èŠ‚è·³åŠ¨ai',
            'ç™¾å·æ™ºèƒ½', 'baichuan',
            'é›¶ä¸€ä¸‡ç‰©', '01ai', 'yiæ¨¡å‹',
            'æ·±åº¦æ±‚ç´¢', 'deepseek',
            'é¢å£æ™ºèƒ½', 'cpm',
            'minimax',
            # å¼€æºæ¨¡å‹
            'å¼€æºå¤§æ¨¡å‹', 'å¼€æºllm', 'å¼€æºæ¨¡å‹',
            'ä¸–ç•Œæ¨¡å‹', 'world model',
            # AIå…¬å¸åŠ¨æ€
            'å‘å¸ƒ', 'ä¸Šçº¿', 'æ¨å‡º', 'å¼€æº', 'æ›´æ–°',
            # AIæŠ€æœ¯å’Œåº”ç”¨
            'gpt-4', 'gpt4', 'claude', 'anthropic',
            'gemini', 'llama', 'meta ai', 'grok',
            'sora', 'midjourney', 'stable diffusion',
            'chatgpt', 'openai',
        ]

        # AIæŠ€æœ¯å…³é”®è¯
        ai_tech_keywords = [
            'å¤§æ¨¡å‹', 'llm', 'å¤§å‹è¯­è¨€æ¨¡å‹',
            'aigc', 'ç”Ÿæˆå¼ai', 'generative ai',
            'transformer', 'attentionæœºåˆ¶',
            'å¤šæ¨¡æ€', 'è§†è§‰æ¨¡å‹', 'è¯­éŸ³æ¨¡å‹',
            'agent', 'aiä»£ç†', 'æ™ºèƒ½ä½“',
            'rag', 'æ£€ç´¢å¢å¼ºç”Ÿæˆ',
            'å¾®è°ƒ', 'fine-tuning', 'è®­ç»ƒ',
        ]

        # åˆå¹¶æ‰€æœ‰å…³é”®è¯ï¼ˆç”¨äºç­›é€‰ï¼‰
        all_keywords = legal_tech_keywords + legal_tech_secondary + ai_major_domestic + ai_tech_keywords

        def calculate_relevance_score(article):
            """è®¡ç®—å•æ¡æ–°é—»çš„ç›¸å…³æ€§å¾—åˆ†"""
            score = 0
            title = str(article.get('title') or '').lower()
            description = str(article.get('description') or '').lower()
            source = article.get('source', {}).get('name', '') or ''

            # 1. æ¥æºæƒé‡ï¼ˆ0-30åˆ†ï¼‰
            source_weight = source_weights.get(source, 1.0)
            score += source_weight * 10

            # 2. å…³é”®è¯åŒ¹é…å¾—åˆ†
            # æ£€æŸ¥æ ‡é¢˜ä¸­çš„å…³é”®è¯åŒ¹é…
            for keyword in all_keywords:
                if keyword in title:
                    # æ³•å¾‹ç§‘æŠ€æ ¸å¿ƒå…³é”®è¯
                    if keyword in legal_tech_keywords:
                        score += 60  # æœ€é«˜ä¼˜å…ˆçº§
                    # AIé‡å¤§æ–°é—»å…³é”®è¯
                    elif keyword in ai_major_domestic:
                        score += 55  # é«˜ä¼˜å…ˆçº§
                    # æ³•å¾‹ç§‘æŠ€æ¬¡è¦å…³é”®è¯
                    elif keyword in legal_tech_secondary:
                        score += 45
                    # AIæŠ€æœ¯å…³é”®è¯
                    elif keyword in ai_tech_keywords:
                        score += 40
                    else:
                        score += 20

                    # å…³é”®è¯åœ¨æ ‡é¢˜å¼€å¤´ï¼ˆå‰50ä¸ªå­—ç¬¦ï¼‰
                    if len(title) > 0 and keyword in title[:50]:
                        score += 10

            # æ£€æŸ¥æè¿°ä¸­çš„å…³é”®è¯åŒ¹é…
            desc_keyword_count = sum(1 for kw in all_keywords if kw in description)
            score += desc_keyword_count * 5  # æ¯ä¸ªå…³é”®è¯+5åˆ†

            # 3. æ—¶é—´æ–°é²œåº¦å¾—åˆ†ï¼ˆ0-20åˆ†ï¼‰
            pub_time = article.get('publishedAt', '')
            if pub_time:
                try:
                    from datetime import timezone, timedelta
                    dt = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)

                    now = datetime.now(timezone.utc)
                    time_diff = (now - dt).total_seconds() / 3600  # å°æ—¶

                    if time_diff <= 24:
                        score += 20  # 24å°æ—¶å†…
                    elif time_diff <= 48:
                        score += 10  # 48å°æ—¶å†…
                    elif time_diff <= 168:  # 7å¤©
                        score += 5
                except:
                    pass

            return score

        def classify_article(article):
            """å°†æ–°é—»åˆ†ç±»ä¸ºï¼šæ³•å¾‹ç§‘æŠ€æ–°é—»ã€AIé‡å¤§æ–°é—»ã€æˆ–å…¶ä»–"""
            title = str(article.get('title') or '').lower()
            description = str(article.get('description') or '').lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ³•å¾‹ç§‘æŠ€å…³é”®è¯
            has_legal_tech = any(kw in title or kw in description for kw in legal_tech_keywords + legal_tech_secondary)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«AIé‡å¤§æ–°é—»å…³é”®è¯
            has_ai_major = any(kw in title or kw in description for kw in ai_major_domestic + ai_tech_keywords)

            if has_legal_tech and has_ai_major:
                return 'both'  # ä¸¤è€…éƒ½æ˜¯
            elif has_legal_tech:
                return 'legal_tech'
            elif has_ai_major:
                return 'ai_major'
            else:
                return 'other'

        # ä¸ºæ¯æ¡æ–°é—»è®¡ç®—å¾—åˆ†ã€åˆ†ç±»å¹¶ä¿ç•™é€šè¿‡åŸºæœ¬ç­›é€‰çš„
        scored_articles = []
        for article in unique_articles:
            title = str(article.get('title') or '').lower()
            description = str(article.get('description') or '').lower()
            pub_time = article.get('publishedAt', '')

            # æ—¶é—´ç­›é€‰ï¼šåªä¿ç•™3å¤©å†…çš„æ–°é—»
            is_recent = True
            if pub_time:
                try:
                    from datetime import timezone, timedelta
                    dt = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)

                    now = datetime.now(timezone.utc)
                    time_diff = (now - dt).total_seconds() / 86400  # è½¬æ¢ä¸ºå¤©æ•°

                    if time_diff > 3:  # è¶…è¿‡3å¤©
                        is_recent = False
                except:
                    pass

            if not is_recent:
                continue  # è·³è¿‡è¶…è¿‡3å¤©çš„æ–°é—»

            # åŸºæœ¬ç­›é€‰ï¼šå¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå…³é”®è¯
            contains_keyword = any(
                kw in title or kw in description
                for kw in all_keywords
            )

            if contains_keyword:
                score = calculate_relevance_score(article)
                category = classify_article(article)
                article['_score'] = score
                article['_category'] = category
                scored_articles.append(article)

        # æŒ‰å¾—åˆ†æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
        scored_articles.sort(key=lambda x: x.get('_score', 0), reverse=True)

        logger.info(f"âœ… è¯„åˆ†åå‰©ä½™ {len(scored_articles)} æ¡ç²¾å‡†æ–°é—»")

        # ç»Ÿè®¡åˆ†ç±»
        legal_tech_count = sum(1 for a in scored_articles if a.get('_category') in ['legal_tech', 'both'])
        ai_major_count = sum(1 for a in scored_articles if a.get('_category') in ['ai_major', 'both'])
        both_count = sum(1 for a in scored_articles if a.get('_category') == 'both')

        logger.info(f"ğŸ“Š æ–°é—»åˆ†ç±»ç»Ÿè®¡ï¼š")
        logger.info(f"   â€¢ æ³•å¾‹ç§‘æŠ€æ–°é—»ï¼š{legal_tech_count} æ¡")
        logger.info(f"   â€¢ AIé‡å¤§æ–°é—»ï¼š{ai_major_count} æ¡")
        if both_count > 0:
            logger.info(f"   â€¢ ä¸¤è€…é‡å ï¼š{both_count} æ¡")

        # æ˜¾ç¤ºå¾—åˆ†æœ€é«˜çš„å‰3æ¡æ–°é—»
        if scored_articles:
            logger.info("\nğŸ† å¾—åˆ†æœ€é«˜çš„æ–°é—»é¢„è§ˆ:")
            for i, article in enumerate(scored_articles[:3], 1):
                score = article.get('_score', 0)
                title = article.get('title', 'æ— æ ‡é¢˜')[:50]
                source = article.get('source', {}).get('name', 'æœªçŸ¥')
                category = article.get('_category', 'unknown')
                logger.info(f"   {i}. [{score}åˆ†] [{category}] {source}: {title}...")

        # ========== ç¬¬å…­æ­¥ï¼šå¤„ç†æ— æ–°é—»çš„æƒ…å†µ ==========
        if len(scored_articles) == 0:
            logger.warning("âš ï¸ ä»Šæ—¥æš‚æ— ç²¾å‡†çš„æ³•å¾‹ç§‘æŠ€/AIç›¸å…³æ–°é—»")
            return [{
                'no_news_message': 'ä»Šæ—¥æš‚æ— ç²¾å‡†çš„æ³•å¾‹ç§‘æŠ€/AIç›¸å…³æ–°é—»'
            }]

        # ========== ç¬¬ä¸ƒæ­¥ï¼šå–å‰Næ¡ï¼ˆæŒ‰ç»¼åˆå¾—åˆ†æ’åºï¼‰==========
        final_articles = scored_articles[:self.config.max_articles]
        logger.info(f"ğŸ¯ æœ€ç»ˆé€‰å– {len(final_articles)} æ¡æ–°é—»ï¼ˆæŒ‰ç»¼åˆå¾—åˆ†æ’åºï¼‰")

        # ä¿ç•™åˆ†ç±»ä¿¡æ¯
        for article in final_articles:
            if '_category' not in article:
                article['_category'] = classify_article(article)

        # æ˜¾ç¤ºæ¥æºåˆ†å¸ƒ
        source_count = {}
        for article in final_articles:
            source = article.get('source', {}).get('name', 'æœªçŸ¥')
            source_count[source] = source_count.get(source, 0) + 1

        logger.info("\nğŸ“Š æ–°é—»æ¥æºåˆ†å¸ƒ:")
        for source, count in source_count.items():
            logger.info(f"   â€¢ {source}: {count} æ¡")

        return final_articles


# ====================== Claude AI å†…å®¹ç”Ÿæˆæ¨¡å— ======================
class NewsletterGenerator:
    """Newsletterç”Ÿæˆç±»ï¼šä½¿ç”¨Claude APIå°†æ–°é—»æ•´ç†æˆä¸­æ–‡æ ¼å¼"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–Newsletterç”Ÿæˆå™¨
        :param config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.session = requests.Session()

    def generate_newsletter(self, articles: List[Dict]) -> str:
        """
        ä½¿ç”¨Claude APIç”Ÿæˆä¸­æ–‡Newsletter
        :param articles: æ–°é—»åˆ—è¡¨
        :return: æ ¼å¼åŒ–åçš„ä¸­æ–‡Newsletteræ–‡æœ¬
        """
        # ========== æ£€æŸ¥æ˜¯å¦æœ‰"æ— æ–°é—»"çš„æ ‡è®° ==========
        if not articles:
            logger.warning("âš ï¸ æ²¡æœ‰æ–°é—»å¯ä¾›æ•´ç†")
            return "ä»Šæ—¥æš‚æ— æ³•å¾‹ç§‘æŠ€ç›¸å…³æ–°é—»"

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ— ç²¾å‡†æ–°é—»çš„ç‰¹æ®Šæ ‡è®°
        if len(articles) == 1 and 'no_news_message' in articles[0]:
            logger.info("ğŸ“­ ä»Šæ—¥æš‚æ— ç²¾å‡†æ–°é—»ï¼Œä½¿ç”¨æç¤ºä¿¡æ¯")
            date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
            return f"""ğŸ“° æ³•å¾‹ç§‘æŠ€æ—¥æŠ¥ - {date_str}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“­ {articles[0]['no_news_message']}

ğŸ’¡ æç¤ºï¼šä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ ‡å‡†çš„æ³•å¾‹ç§‘æŠ€/æ³•å¾‹AIç›¸å…³æ–°é—»ã€‚è¿™å¯èƒ½æ˜¯ç”±äºï¼š
  â€¢ å½“å¤©ç›¸å…³æ–°é—»å‘å¸ƒè¾ƒå°‘
  â€¢ æ–°é—»æ¥æºæœªè¦†ç›–æ‰€æœ‰æ¸ é“
  â€¢ å…³é”®è¯åŒ¹é…éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¤– ç”±æ³•å¾‹ç§‘æŠ€æ–°é—»Botè‡ªåŠ¨æ¨é€"""

        # å¦‚æœæ²¡æœ‰Claude APIå¯†é’¥ï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        if not self.config.claude_api_key:
            logger.info("ğŸ’¡ æœªé…ç½®Claude APIï¼Œä½¿ç”¨ç®€å•æ ¼å¼")
            return self._fallback_newsletter(articles)

        logger.info("ğŸ¤– å¼€å§‹ä½¿ç”¨Claude APIç”ŸæˆNewsletter...")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        if not self.config.enable_fallback:
            logger.info("ğŸ“Œ å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨ï¼Œä»…ä½¿ç”¨Claude API")

        try:
            # æ„å»ºå‘é€ç»™Claudeçš„æ–°é—»æ‘˜è¦
            news_summary = self._prepare_news_summary(articles)

            # æ„å»ºClaude APIè¯·æ±‚
            headers = {
                'x-api-key': self.config.claude_api_key,
                'anthropic-version': '2023-06-01',
                'content-type': 'application/json'
            }

            # Claude APIè¯·æ±‚ä½“
            payload = {
                'model': 'claude-3-5-haiku-20241022',  # ä½¿ç”¨Haikuæ¨¡å‹ï¼Œæˆæœ¬ä½
                'max_tokens': self.config.claude_max_tokens,  # æœ€å¤§è¿”å›tokenæ•°
                'messages': [{
                    'role': 'user',
                    'content': self._build_prompt(news_summary)
                }]
            }

            # å‘é€è¯·æ±‚åˆ°Claude API
            response = self.session.post(
                self.config.claude_api_url,
                headers=headers,
                json=payload,
                timeout=30  # 30ç§’è¶…æ—¶
            )

            # æ£€æŸ¥å“åº”çŠ¶æ€
            response.raise_for_status()

            # è§£æå“åº”
            result = response.json()
            newsletter_content = result['content'][0]['text']

            logger.info("âœ… Newsletterç”ŸæˆæˆåŠŸï¼ˆä½¿ç”¨Claude APIï¼‰")
            return newsletter_content

        except requests.exceptions.HTTPError as e:
            logger.error(f"âŒ Claude APIè¯·æ±‚å¤±è´¥: {e}")
            if e.response.status_code == 401:
                logger.error("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥CLAUDE_API_KEYæ˜¯å¦æ­£ç¡®")
            elif e.response.status_code == 429:
                logger.error("ğŸ’¡ æç¤ºï¼šè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•")
            elif e.response.status_code == 400:
                logger.error("ğŸ’¡ æç¤ºï¼šè¯·æ±‚å‚æ•°é”™è¯¯æˆ–APIä½™é¢ä¸è¶³")

            # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if self.config.enable_fallback:
                logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆ...")
                return self._fallback_newsletter(articles)
            else:
                logger.error("âŒ å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨ï¼Œæ— æ³•ç”ŸæˆNewsletter")
                return "æŠ±æ­‰ï¼ŒNewsletterç”Ÿæˆå¤±è´¥ï¼Œä¸”å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨"

        except requests.exceptions.Timeout:
            logger.error("âŒ Claude APIè¯·æ±‚è¶…æ—¶")
            if self.config.enable_fallback:
                logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆ...")
                return self._fallback_newsletter(articles)
            else:
                return "æŠ±æ­‰ï¼ŒAPIè¯·æ±‚è¶…æ—¶ï¼Œä¸”å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨"

        except Exception as e:
            logger.error(f"âŒ Newsletterç”Ÿæˆå¤±è´¥: {e}")
            if self.config.enable_fallback:
                logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆ...")
                return self._fallback_newsletter(articles)
            else:
                return "æŠ±æ­‰ï¼ŒNewsletterç”Ÿæˆå¤±è´¥ï¼Œä¸”å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨"

    def _prepare_news_summary(self, articles: List[Dict]) -> str:
        """
        å°†æ–°é—»åˆ—è¡¨æ ¼å¼åŒ–ä¸ºæ–‡æœ¬æ‘˜è¦
        :param articles: æ–°é—»åˆ—è¡¨
        :return: æ ¼å¼åŒ–çš„æ–‡æœ¬æ‘˜è¦
        """
        summary_lines = []
        for i, article in enumerate(articles, 1):
            title = article.get('title', 'æ— æ ‡é¢˜')
            description = article.get('description', 'æ— æè¿°')
            url = article.get('url', '')
            source = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
            published_at = article.get('publishedAt', '')

            summary_lines.append(
                f"{i}. æ ‡é¢˜: {title}\n"
                f"   æ¥æº: {source}\n"
                f"   æè¿°: {description}\n"
                f"   é“¾æ¥: {url}\n"
            )

        return '\n'.join(summary_lines)

    def _build_prompt(self, news_summary: str) -> str:
        """
        æ„å»ºå‘é€ç»™Claudeçš„æç¤ºè¯
        :param news_summary: æ–°é—»æ‘˜è¦
        :return: å®Œæ•´çš„æç¤ºè¯
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹ç§‘æŠ€æ–°é—»ç¼–è¾‘ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–°é—»æ•´ç†æˆä¸€ä»½ç®€æ˜æ‰¼è¦çš„ä¸­æ–‡Newsletterã€‚

è¦æ±‚ï¼š
1. æ ‡é¢˜ï¼šä½¿ç”¨å¸å¼•äººçš„æ ‡é¢˜ï¼ŒåŒ…å«æ—¥æœŸ
2. æ ¼å¼ï¼šæ¯æ¡æ–°é—»åŒ…å«ã€æ ‡é¢˜ã€‘ã€ã€ç®€çŸ­æ‘˜è¦ã€‘ã€ã€æ¥æºã€‘ã€ã€é“¾æ¥ã€‘
3. è¯­è¨€ï¼šå…¨éƒ¨ä½¿ç”¨ä¸­æ–‡
4. é£æ ¼ï¼šç®€æ´ä¸“ä¸šï¼Œé€‚åˆå¿«é€Ÿé˜…è¯»
5. æœ€å¤šæ•´ç†8æ¡æœ€é‡è¦æ–°é—»
6. ä½¿ç”¨emojiè®©ç‰ˆé¢æ›´ç”ŸåŠ¨

ä»¥ä¸‹æ˜¯ä»Šå¤©çš„æ–°é—»ï¼š

{news_summary}

è¯·ç›´æ¥è¾“å‡ºNewsletterå†…å®¹ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šã€‚"""

        return prompt

    def _clean_html(self, html_text: str) -> str:
        """
        æ¸…ç†HTMLæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬
        :param html_text: åŒ…å«HTMLæ ‡ç­¾çš„æ–‡æœ¬
        :return: æ¸…ç†åçš„çº¯æ–‡æœ¬
        """
        if not html_text:
            return html_text

        # ç§»é™¤HTMLæ ‡ç­¾
        # ç§»é™¤<script>æ ‡ç­¾åŠå…¶å†…å®¹
        clean_text = re.sub(r'<script.*?>.*?</script>', '', html_text, flags=re.DOTALL | re.IGNORECASE)
        # ç§»é™¤<style>æ ‡ç­¾åŠå…¶å†…å®¹
        clean_text = re.sub(r'<style.*?>.*?</style>', '', clean_text, flags=re.DOTALL | re.IGNORECASE)
        # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾
        clean_text = re.sub(r'<[^>]+>', '', clean_text)
        # ç§»é™¤HTMLå®ä½“ï¼ˆå¦‚&nbsp;, &lt;, &gt;ç­‰ï¼‰
        clean_text = re.sub(r'&nbsp;', ' ', clean_text)
        clean_text = re.sub(r'&lt;', '<', clean_text)
        clean_text = re.sub(r'&gt;', '>', clean_text)
        clean_text = re.sub(r'&amp;', '&', clean_text)
        clean_text = re.sub(r'&quot;', '"', clean_text)
        clean_text = re.sub(r'&#39;', "'", clean_text)
        clean_text = re.sub(r'&apos;', "'", clean_text)
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text)
        # å»é™¤é¦–å°¾ç©ºæ ¼
        clean_text = clean_text.strip()

        return clean_text

    def _translate_text(self, text: str, max_length: int = 5000) -> str:
        """
        ä½¿ç”¨å…è´¹çš„Googleç¿»è¯‘ç¿»è¯‘æ–‡æœ¬
        :param text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        :param max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼ˆGoogleç¿»è¯‘é™åˆ¶ï¼‰
        :return: ç¿»è¯‘åçš„æ–‡æœ¬
        """
        if not text or not text.strip():
            return text

        try:
            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œæˆªæ–­ç¿»è¯‘
            if len(text) > max_length:
                text = text[:max_length] + "..."

            translator = GoogleTranslator(source='auto', target='zh-CN')
            translated = translator.translate(text)
            return translated
        except Exception as e:
            logger.warning(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}ï¼Œä¿ç•™åŸæ–‡")
            return text

    def _fallback_newsletter(self, articles: List[Dict]) -> str:
        """
        å¤‡ç”¨æ–¹æ¡ˆï¼šå½“Claude APIè°ƒç”¨å¤±è´¥æ—¶ï¼Œä½¿ç”¨ç®€å•çš„æ ¼å¼åŒ–
        ç°åœ¨åŒ…å«å…è´¹ç¿»è¯‘åŠŸèƒ½ï¼Œå¹¶æ”¯æŒåˆ†ç±»å±•ç¤º
        :param articles: æ–°é—»åˆ—è¡¨
        :return: ç®€å•æ ¼å¼åŒ–çš„æ–°é—»æ–‡æœ¬
        """
        logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆç”ŸæˆNewsletterï¼ˆåŒ…å«å…è´¹ç¿»è¯‘å’Œåˆ†ç±»å±•ç¤ºï¼‰")

        date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        lines = [
            f"ğŸ“° æ³•å¾‹ç§‘æŠ€ä¸AIæ—¥æŠ¥ - {date_str}",
            "",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            ""
        ]

        # æŒ‰åˆ†ç±»æ•´ç†æ–°é—»
        legal_tech_articles = []
        ai_major_articles = []
        other_articles = []

        for article in articles[:self.config.max_articles]:
            category = article.get('_category', 'other')
            if category == 'legal_tech':
                legal_tech_articles.append(article)
            elif category == 'ai_major':
                ai_major_articles.append(article)
            elif category == 'both':
                # ä¸¤è€…éƒ½æœ‰çš„ï¼Œä¼˜å…ˆæ”¾åˆ°æ³•å¾‹ç§‘æŠ€ç±»
                legal_tech_articles.append(article)
            else:
                other_articles.append(article)

        # æ·»åŠ ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥æ ¼å¼åŒ–å•æ¡æ–°é—»
        def format_article(article, index):
            title = article.get('title', 'æ— æ ‡é¢˜')
            description = article.get('description', '')
            url = article.get('url', '')
            source = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
            published_at = article.get('publishedAt', '')

            # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
            publish_time = ''
            if published_at:
                try:
                    dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    publish_time = dt.strftime('%Y-%m-%d %H:%M')
                except:
                    publish_time = published_at

            # ç¿»è¯‘æ ‡é¢˜å’Œæè¿°ï¼ˆå…ˆæ¸…ç†HTMLæ ‡ç­¾ï¼‰
            try:
                title_clean = self._clean_html(title)
                description_clean = self._clean_html(description) if description else ""

                # è®¡ç®—æ ‡é¢˜å’Œæ‘˜è¦çš„ç›¸ä¼¼åº¦
                should_show_description = False
                if description_clean:
                    title_normalized = title_clean.lower()
                    desc_normalized = description_clean.lower()
                    translator = str.maketrans('', '', string.punctuation + ' ')
                    title_normalized = title_normalized.translate(translator)
                    desc_normalized = desc_normalized.translate(translator)
                    similarity = SequenceMatcher(None, title_normalized, desc_normalized).ratio()
                    if similarity < 0.95:
                        should_show_description = True

                title_translated = self._translate_text(title_clean)
                description_translated = self._translate_text(description_clean) if should_show_description else ""

                result = [
                    f"ğŸ“Œ [{index}] {title_translated}",
                ]
                if description_translated:
                    result.append(f"    {description_translated}")
                result.append(f"    ğŸ“ æ¥æº: {source}")
                if publish_time:
                    result.append(f"    ğŸ•’ å‘å¸ƒæ—¶é—´: {publish_time}")
                result.append(f"    ğŸ”— é“¾æ¥: {url}")
                return result
            except Exception as e:
                logger.warning(f"âš ï¸ ç¿»è¯‘æ–°é—»å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ–‡")
                result = [f"ğŸ“Œ [{index}] {title}"]
                if description and str(description).lower() != str(title).lower():
                    result.append(f"    {description}")
                result.append(f"    ğŸ“ æ¥æº: {source}")
                if publish_time:
                    result.append(f"    ğŸ•’ å‘å¸ƒæ—¶é—´: {publish_time}")
                result.append(f"    ğŸ”— é“¾æ¥: {url}")
                return result

        # ========== ç¬¬ä¸€éƒ¨åˆ†ï¼šæ³•å¾‹ç§‘æŠ€æ–°é—» ==========
        if legal_tech_articles:
            lines.append("ğŸ”– ã€æ³•å¾‹ç§‘æŠ€æ–°é—»ã€‘")
            lines.append("")
            for i, article in enumerate(legal_tech_articles[:8], 1):  # æœ€å¤š8æ¡
                lines.extend(format_article(article, i))
                lines.append("")

        # ========== ç¬¬äºŒéƒ¨åˆ†ï¼šAIé‡å¤§æ–°é—» ==========
        if ai_major_articles:
            lines.append("ğŸ¤– ã€AIé‡å¤§æ–°é—»ã€‘")
            lines.append("")
            for i, article in enumerate(ai_major_articles[:8], 1):  # æœ€å¤š8æ¡
                lines.extend(format_article(article, i))
                lines.append("")

        # ========== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå…¶ä»–ç›¸å…³æ–°é—» ==========
        if other_articles and len(legal_tech_articles) + len(ai_major_articles) < 10:
            lines.append("ğŸ“° ã€å…¶ä»–ç›¸å…³æ–°é—»ã€‘")
            lines.append("")
            for i, article in enumerate(other_articles[:5], 1):  # æœ€å¤š5æ¡
                lines.extend(format_article(article, i))
                lines.append("")

        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("")
        lines.append("ğŸ¤– ç”±æ³•å¾‹ç§‘æŠ€æ–°é—»Botè‡ªåŠ¨æ¨é€ï¼ˆä½¿ç”¨å…è´¹ç¿»è¯‘ï¼‰")

        return '\n'.join(lines)


# ====================== é£ä¹¦æ¨é€æ¨¡å— ======================
class FeishuNotifier:
    """é£ä¹¦é€šçŸ¥ç±»ï¼šé€šè¿‡Webhookå‘é€æ¶ˆæ¯åˆ°é£ä¹¦ç¾¤"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–é£ä¹¦é€šçŸ¥å™¨
        :param config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.session = requests.Session()

    def send_newsletter(self, newsletter_content: str):
        """
        å‘é€Newsletteråˆ°é£ä¹¦ç¾¤
        :param newsletter_content: Newsletterå†…å®¹
        """
        logger.info("ğŸ“¤ å¼€å§‹å‘é€é£ä¹¦é€šçŸ¥...")

        try:
            # æ„å»ºé£ä¹¦æ¶ˆæ¯æ ¼å¼
            message = {
                "msg_type": "text",
                "content": {
                    "text": newsletter_content
                }
            }

            # å‘é€POSTè¯·æ±‚åˆ°é£ä¹¦Webhook
            response = self.session.post(
                self.config.feishu_webhook,
                json=message,
                timeout=10
            )

            # æ£€æŸ¥å“åº”çŠ¶æ€
            response.raise_for_status()

            result = response.json()

            # æ£€æŸ¥é£ä¹¦APIè¿”å›ç 
            if result.get('code') == 0:
                logger.info("âœ… é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                logger.error(f"âŒ é£ä¹¦APIè¿”å›é”™è¯¯: {result}")

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e}")


# ====================== Botä¸»æ§åˆ¶å™¨ ======================
class LegalTechNewsBot:
    """æ³•å¾‹ç§‘æŠ€æ–°é—»Botä¸»æ§åˆ¶å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–Bot"""
        logger.info("=" * 60)
        logger.info("ğŸš€ æ³•å¾‹ç§‘æŠ€æ–°é—»Botå¯åŠ¨")
        logger.info("=" * 60)

        # åˆå§‹åŒ–é…ç½®
        self.config = Config()

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.news_fetcher = NewsFetcher(self.config)
        self.newsletter_generator = NewsletterGenerator(self.config)
        self.feishu_notifier = FeishuNotifier(self.config)

    def run_daily_task(self):
        """
        æ‰§è¡Œæ¯æ—¥ä»»åŠ¡ï¼šæŠ“å–æ–°é—» -> ç”ŸæˆNewsletter -> æ¨é€åˆ°é£ä¹¦
        """
        logger.info("\n" + "=" * 60)
        logger.info(f"â° å¼€å§‹æ‰§è¡Œæ¯æ—¥ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60 + "\n")

        try:
            # æ­¥éª¤1: è·å–æ–°é—»
            articles = self.news_fetcher.fetch_legal_tech_news()

            if not articles:
                logger.warning("âš ï¸ æœªè·å–åˆ°æ–°é—»ï¼Œä»»åŠ¡ç»“æŸ")
                return

            # æ­¥éª¤2: ç”ŸæˆNewsletter
            newsletter = self.newsletter_generator.generate_newsletter(articles)

            # æ­¥éª¤3: å‘é€åˆ°é£ä¹¦
            self.feishu_notifier.send_newsletter(newsletter)

            logger.info("\n" + "=" * 60)
            logger.info("âœ… ä»Šæ—¥ä»»åŠ¡å®Œæˆ")
            logger.info("=" * 60 + "\n")

        except Exception as e:
            logger.error(f"\nâŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            logger.error("=" * 60 + "\n")

    def start(self):
        """
        å¯åŠ¨Botï¼šè®¾ç½®å®šæ—¶ä»»åŠ¡ï¼Œæ¯å¤©ä¸­åˆ12ç‚¹æ‰§è¡Œ
        """
        # è®¾ç½®æ¯å¤©12:00æ‰§è¡Œä»»åŠ¡
        schedule.every().day.at("12:00").do(self.run_daily_task)

        logger.info("ğŸ“… Botå·²å¯åŠ¨ï¼Œå°†åœ¨æ¯å¤©ä¸­åˆ12:00æ¨é€æ–°é—»")
        logger.info("ğŸ’¡ æç¤ºï¼šæŒ‰Ctrl+Cå¯ä»¥åœæ­¢Bot\n")

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•ï¼‰
        # self.run_daily_task()

        # æŒç»­è¿è¡Œ
        try:
            while True:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡
                schedule.run_pending()
                # æ¯éš”60ç§’æ£€æŸ¥ä¸€æ¬¡
                time.sleep(60)

        except KeyboardInterrupt:
            logger.info("\nğŸ‘‹ Botå·²åœæ­¢")

    def run_once(self):
        """
        å•æ¬¡è¿è¡Œæ¨¡å¼ï¼šç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        logger.info("ğŸ§ª å•æ¬¡è¿è¡Œæ¨¡å¼")
        self.run_daily_task()


# ====================== ä¸»ç¨‹åºå…¥å£ ======================
def main():
    """
    ä¸»å‡½æ•°ï¼šç¨‹åºå…¥å£
    æ”¯æŒä¸¤ç§è¿è¡Œæ¨¡å¼ï¼š
    1. testæ¨¡å¼ï¼šç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    2. productionæ¨¡å¼ï¼šå®šæ—¶è¿è¡Œï¼Œæ¯å¤©12ç‚¹æ‰§è¡Œï¼ˆç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

    æ¨¡å¼åˆ‡æ¢ï¼šåœ¨.envæ–‡ä»¶ä¸­è®¾ç½® RUN_MODE=test æˆ– RUN_MODE=production
    """
    try:
        # åˆ›å»ºBotå®ä¾‹
        bot = LegalTechNewsBot()

        # æ ¹æ®é…ç½®é€‰æ‹©è¿è¡Œæ¨¡å¼
        if bot.config.run_mode == 'test':
            logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚éœ€åˆ‡æ¢åˆ°å®šæ—¶æ¨¡å¼ï¼Œè¯·åœ¨.envä¸­è®¾ç½® RUN_MODE=production")
            bot.run_once()
        else:
            logger.info("ğŸš€ ç”Ÿäº§æ¨¡å¼ï¼šå®šæ—¶è¿è¡Œä¸­")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚éœ€åˆ‡æ¢åˆ°æµ‹è¯•æ¨¡å¼ï¼Œè¯·åœ¨.envä¸­è®¾ç½® RUN_MODE=test")
            bot.start()

    except ValueError as e:
        logger.error(f"âŒ é…ç½®é”™è¯¯: {e}")
        logger.error("è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„é…ç½®é¡¹æ˜¯å¦æ­£ç¡®")

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")


# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œåˆ™æ‰§è¡Œmainå‡½æ•°
if __name__ == '__main__':
    main()
