#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³•å¾‹ç§‘æŠ€æ–°é—»æ±‡æ€»Bot
åŠŸèƒ½ï¼šæ¯å¤©è‡ªåŠ¨æŠ“å–Legal Techæ–°é—»ï¼Œç”¨AIæ•´ç†æˆä¸­æ–‡Newsletterï¼Œé€šè¿‡é£ä¹¦æ¨é€
ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025
"""

import os        # ç”¨äºè¯»å–ç¯å¢ƒå˜é‡
import json      # ç”¨äºå¤„ç†JSONæ•°æ®
import time      # ç”¨äºæ—¶é—´å¤„ç†å’Œç­‰å¾…
import schedule  # ç”¨äºå®šæ—¶ä»»åŠ¡
import logging   # ç”¨äºæ—¥å¿—è®°å½•
from datetime import datetime
from typing import List, Dict

import requests      # ç”¨äºHTTPè¯·æ±‚
from dotenv import load_dotenv  # ç”¨äºåŠ è½½.envé…ç½®æ–‡ä»¶
from deep_translator import GoogleTranslator  # ç”¨äºå…è´¹ç¿»è¯‘
import feedparser      # ç”¨äºRSSè§£æ
import re              # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ¸…ç†HTML
import string          # ç”¨äºå­—ç¬¦ä¸²å¤„ç†
from difflib import SequenceMatcher  # ç”¨äºè®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦

# ====================== é…ç½®æ—¥å¿—ç³»ç»Ÿ ======================
# æ—¥å¿—ä¼šè¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°ï¼Œæ–¹ä¾¿è°ƒè¯•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('news_bot.log', encoding='utf-8'),  # è¾“å‡ºåˆ°æ–‡ä»¶
        logging.StreamHandler()  # è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)
logger = logging.getLogger(__name__)


# ====================== é…ç½®ç®¡ç† ======================
class Config:
    """é…ç½®ç±»ï¼šç»Ÿä¸€ç®¡ç†æ‰€æœ‰é…ç½®é¡¹"""

    def __init__(self):
        """åŠ è½½.envæ–‡ä»¶ä¸­çš„é…ç½®"""
        # åŠ è½½.envæ–‡ä»¶
        load_dotenv()

        # ========== å¿…éœ€é…ç½® ==========
        # NewsAPIé…ç½®
        self.news_api_key = os.getenv('NEWS_API_KEY')
        self.news_api_url = 'https://newsapi.org/v2/everything'
        # NewsAPIå…è´¹ç‰ˆé™åˆ¶ï¼šæ¯å¤©æœ€å¤š100æ¬¡è¯·æ±‚
        self.news_api_daily_limit = 100  # å…è´¹ç‰ˆæ¯æ—¥é™åˆ¶
        self.news_api_request_count = 0  # å½“æ—¥å·²ä½¿ç”¨æ¬¡æ•°

        # Claude APIé…ç½®
        self.claude_api_key = os.getenv('CLAUDE_API_KEY')
        self.claude_api_url = 'https://api.anthropic.com/v1/messages'
        # Claude Haikuä»·æ ¼ï¼šçº¦$0.25/ç™¾ä¸‡è¾“å…¥tokenï¼Œ$1.25/ç™¾ä¸‡è¾“å‡ºtoken
        self.claude_max_tokens = 2000  # æ¯æ¬¡è¯·æ±‚æœ€å¤§tokenæ•°

        # é£ä¹¦æœºå™¨äººé…ç½®
        self.feishu_webhook = os.getenv('FEISHU_WEBHOOK_URL')

        # ========== å¯é€‰é…ç½® ==========
        # æ–°é—»æœç´¢å…³é”®è¯ï¼ˆè‹±æ–‡ï¼‰
        self.search_keywords = os.getenv('SEARCH_KEYWORDS',
                                       'legal tech OR legal technology OR lawtech OR legal AI')

        # ä¸­æ–‡æ–°é—»æœç´¢å…³é”®è¯ï¼ˆç”¨äºè·å–å›½å†…æ–°é—»ï¼‰
        self.search_keywords_cn = os.getenv('SEARCH_KEYWORDS_CN',
                                           'æ³•å¾‹ç§‘æŠ€ OR æ³•å¾‹AI OR æ³•åŠ¡AI OR æ™ºèƒ½æ³•åŠ¡ OR æ³•å¾‹å¤§æ¨¡å‹ OR æ³•å¾‹äººå·¥æ™ºèƒ½')

        # è¿è¡Œæ¨¡å¼ï¼štestï¼ˆæµ‹è¯•æ¨¡å¼ï¼Œç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼‰æˆ– productionï¼ˆç”Ÿäº§æ¨¡å¼ï¼Œå®šæ—¶è¿è¡Œï¼‰
        self.run_mode = os.getenv('RUN_MODE', 'production').lower()

        # æ˜¯å¦å¯ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼ˆå½“Claude APIå¤±è´¥æ—¶ï¼‰
        self.enable_fallback = os.getenv('ENABLE_FALLBACK', 'true').lower() == 'true'

        # æ–°é—»æ•°é‡é™åˆ¶
        self.max_articles = int(os.getenv('MAX_ARTICLES', '15'))

        # ========== RSSæ–°é—»æºé…ç½® ==========
        # 1. Google News RSSï¼ˆå¤šè¯­è¨€ï¼ŒåŸºäºå…³é”®è¯æœç´¢ï¼‰
        self.rss_google_news = 'https://news.google.com/rss/search?q=legal+tech+OR+legal+AI+OR+æ³•å¾‹ç§‘æŠ€&hl=zh-CN&gl=CN&ceid=CN:zh-Hans'

        # 2. JDSupra Legal Tech RSSï¼ˆè‹±æ–‡ï¼Œç§‘æŠ€ç›¸å…³æ³•å¾‹æ–°é—»ï¼‰
        self.rss_jdsupra = 'https://www.jdsupra.com/resources/syndication/docsRSSfeed.aspx?ftype=ScienceComputersTechnology&premium=1'

        # 3. Artificial Lawyer RSSï¼ˆè‹±æ–‡ï¼Œæ³•å¾‹AIä¸“ä¸šåšå®¢ï¼‰
        self.rss_artificial_lawyer = 'https://www.artificiallawyer.com/feed/'

        # 4. Above the Law RSSï¼ˆè‹±æ–‡ï¼Œæ³•å¾‹æ–°é—»ï¼‰
        self.rss_above_the_law = 'https://abovethelaw.com/feed/'

        # 5. TechLaw RSSï¼ˆè‹±æ–‡ï¼Œæ³•å¾‹ç§‘æŠ€ï¼‰
        self.rss_techlaw = 'https://feeds.feedburner.com/techlaw'

        # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        self._validate_config()
        self._log_api_limits()

    def _validate_config(self):
        """éªŒè¯æ‰€æœ‰å¿…éœ€çš„é…ç½®æ˜¯å¦å­˜åœ¨"""
        required_configs = {
            'NEWS_API_KEY': self.news_api_key,
            'FEISHU_WEBHOOK_URL': self.feishu_webhook
        }

        # Claude APIå¯†é’¥æ˜¯å¯é€‰çš„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç®€å•æ ¼å¼
        optional_configs = {
            'CLAUDE_API_KEY': self.claude_api_key
        }

        missing_configs = [name for name, value in required_configs.items() if not value]

        if missing_configs:
            raise ValueError(
                f"ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: {', '.join(missing_configs)}\n"
                f"è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®è¿™äº›é¡¹"
            )

        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        logger.info(f"ğŸ“Š è¿è¡Œæ¨¡å¼: {self.run_mode}")

        # æç¤ºClaude APIçŠ¶æ€
        if not self.claude_api_key:
            logger.info("ğŸ’¡ æœªé…ç½®Claude APIï¼Œå°†ä½¿ç”¨ç®€å•æ ¼å¼ï¼ˆè‹±æ–‡åŸæ–°é—»ï¼‰")
        else:
            logger.info("âœ… å·²é…ç½®Claude APIï¼Œå°†ç”Ÿæˆä¸­æ–‡Newsletter")

    def _log_api_limits(self):
        """è®°å½•APIé™åˆ¶ä¿¡æ¯"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ APIé™åˆ¶è¯´æ˜")
        logger.info("=" * 60)
        logger.info(f"NewsAPI: å…è´¹ç‰ˆæ¯å¤©æœ€å¤š {self.news_api_daily_limit} æ¬¡è¯·æ±‚")

        if self.claude_api_key:
            logger.info(f"Claude API: Haikuæ¨¡å‹ï¼Œçº¦$0.25/ç™¾ä¸‡è¾“å…¥token")
            logger.info(f"å½“å‰é…ç½®: æœ€å¤§{self.claude_max_tokens} tokens/æ¬¡")
        else:
            logger.info("Claude API: æœªé…ç½®ï¼ˆä½¿ç”¨å…è´¹ç®€å•æ ¼å¼ï¼‰")

        logger.info(f"å¤‡ç”¨æ–¹æ¡ˆ: {'âœ… å·²å¯ç”¨' if self.enable_fallback else 'âŒ å·²ç¦ç”¨'}")
        logger.info("=" * 60 + "\n")


# ====================== å¤šæºæ–°é—»è·å–æ¨¡å— ======================
class NewsFetcher:
    """æ–°é—»è·å–ç±»ï¼šä»å¤šä¸ªæ¥æºï¼ˆNewsAPI + RSSï¼‰è·å–æ³•å¾‹ç§‘æŠ€æ–°é—»"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–æ–°é—»è·å–å™¨
        :param config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.session = requests.Session()  # ä½¿ç”¨Sessionå¯ä»¥æé«˜HTTPè¯·æ±‚æ•ˆç‡

    def _fetch_from_rss(self, rss_url: str, source_name: str, max_items: int = 10) -> List[Dict]:
        """
        ä»RSSæºè·å–æ–°é—»
        :param rss_url: RSSé“¾æ¥
        :param source_name: æºåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        :param max_items: æœ€å¤§è·å–æ•°é‡
        :return: æ–°é—»åˆ—è¡¨
        """
        articles = []
        try:
            logger.info(f"ğŸ“¡ æ­£åœ¨è·å– {source_name} RSS...")
            feed = feedparser.parse(rss_url)

            if feed.bozo:
                logger.warning(f"âš ï¸ {source_name} RSSè§£æå¯èƒ½æœ‰è¯¯: {feed.bozo_exception}")

            if not feed.entries:
                logger.warning(f"âš ï¸ {source_name} RSSæ²¡æœ‰è¿”å›ä»»ä½•å†…å®¹")
                return articles

            # è§£æRSSæ¡ç›®
            for entry in feed.entries[:max_items]:
                # æå–å‘å¸ƒæ—¶é—´
                published_at = ''
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    try:
                        published_at = datetime(*entry.published_parsed[:6]).isoformat()
                    except:
                        pass
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    try:
                        published_at = datetime(*entry.updated_parsed[:6]).isoformat()
                    except:
                        pass

                # æ„å»ºæ ‡å‡†åŒ–çš„æ–°é—»æ ¼å¼
                # æ¸…ç†descriptionä¸­çš„HTMLæ ‡ç­¾
                description_raw = entry.get('description', '')
                title_raw = entry.get('title', 'æ— æ ‡é¢˜')

                # æ¸…ç†HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºæ ¼
                description_clean = re.sub(r'<[^>]+>', '', description_raw)
                description_clean = re.sub(r'\s+', ' ', description_clean).strip()

                # å¦‚æœdescriptionä¸ºç©ºæˆ–ä¸æ ‡é¢˜ç›¸åŒï¼Œå°è¯•ä»contentä¸­è·å–
                if not description_clean or description_clean.lower() == title_raw.lower():
                    if hasattr(entry, 'content') and entry.get('content'):
                        content_raw = entry.get('content', [{}])[0].get('value', '')
                        description_clean = re.sub(r'<[^>]+>', '', content_raw)
                        description_clean = re.sub(r'\s+', ' ', description_clean).strip()

                        # é™åˆ¶é•¿åº¦ï¼ˆå–å‰200ä¸ªå­—ç¬¦ï¼‰
                        if len(description_clean) > 200:
                            description_clean = description_clean[:200] + '...'

                    # å¦‚æœcontentä¹Ÿæ²¡æœ‰æˆ–è€…è¿˜æ˜¯å’Œæ ‡é¢˜ä¸€æ ·ï¼Œå°±ç•™ç©º
                    if not description_clean or description_clean.lower() == title_raw.lower():
                        description_clean = ''

                article = {
                    'title': title_raw,
                    'description': description_clean,
                    'url': entry.get('link', ''),
                    'source': {'name': source_name},
                    'publishedAt': published_at,
                    'content': entry.get('content', [{}])[0].get('value', '') if hasattr(entry, 'content') else ''
                }
                articles.append(article)

            logger.info(f"âœ… {source_name} RSSè·å– {len(articles)} æ¡")

        except Exception as e:
            logger.error(f"âŒ è·å– {source_name} RSSå¤±è´¥: {e}")

        return articles

    def fetch_legal_tech_news(self) -> List[Dict]:
        """
        ä»å¤šä¸ªæ¥æºï¼ˆ5ä¸ªRSS + NewsAPIï¼‰è·å–æ³•å¾‹ç§‘æŠ€æ–°é—»
        :return: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«æ ‡é¢˜ã€æè¿°ã€URLã€æ¥æºç­‰
        """
        logger.info("ğŸ” å¼€å§‹è·å–æ³•å¾‹ç§‘æŠ€æ–°é—»ï¼ˆå¤šæºæ¨¡å¼ï¼‰...")

        all_articles = []

        # ========== ç¬¬ä¸€æ­¥ï¼šä»5ä¸ªRSSæºè·å–æ–°é—»ï¼ˆå…è´¹ï¼Œæ— é™åˆ¶ï¼‰==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“¡ å¼€å§‹è·å–RSSæ–°é—»æºï¼ˆ5ä¸ªæºï¼‰...")
        logger.info("=" * 60)

        # 1. Google News RSS
        google_news_articles = self._fetch_from_rss(
            self.config.rss_google_news,
            'Google News',
            max_items=20
        )
        all_articles.extend(google_news_articles)

        # 2. JDSupra Legal Tech RSS
        jdsupra_articles = self._fetch_from_rss(
            self.config.rss_jdsupra,
            'JDSupra Legal Tech',
            max_items=20
        )
        all_articles.extend(jdsupra_articles)

        # 3. Artificial Lawyer RSS
        artificial_lawyer_articles = self._fetch_from_rss(
            self.config.rss_artificial_lawyer,
            'Artificial Lawyer',
            max_items=20
        )
        all_articles.extend(artificial_lawyer_articles)

        # 4. Above the Law RSS
        above_the_law_articles = self._fetch_from_rss(
            self.config.rss_above_the_law,
            'Above the Law',
            max_items=20
        )
        all_articles.extend(above_the_law_articles)

        # 5. TechLaw RSS
        techlaw_articles = self._fetch_from_rss(
            self.config.rss_techlaw,
            'TechLaw',
            max_items=10
        )
        all_articles.extend(techlaw_articles)

        logger.info(f"\nâœ… RSSæºå…±è·å– {len(all_articles)} æ¡æ–°é—»")

        # ========== ç¬¬äºŒæ­¥ï¼šä»NewsAPIè·å–æ–°é—»ï¼ˆè¡¥å……ï¼‰==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“° å¼€å§‹è·å–NewsAPIæ–°é—»...")
        logger.info("=" * 60)

        # æ£€æŸ¥APIé…é¢
        self.config.news_api_request_count += 1
        remaining_requests = self.config.news_api_daily_limit - self.config.news_api_request_count

        if remaining_requests <= 0:
            logger.warning(f"âš ï¸ NewsAPIé…é¢å·²ç”¨å®Œï¼Œä»…ä½¿ç”¨RSSæº")
        elif remaining_requests <= 10:
            logger.warning(f"âš ï¸ NewsAPIé…é¢å³å°†ç”¨å°½ï¼šå‰©ä½™ {remaining_requests}/{self.config.news_api_daily_limit} æ¬¡")

        try:
            # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆå‰3å¤©ï¼‰
            from datetime import timedelta
            from_date = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
            today = datetime.now().strftime('%Y-%m-%d')
            logger.info(f"ğŸ“… æœç´¢æ—¥æœŸèŒƒå›´: {from_date} è‡³ {today}ï¼ˆè¿‘3å¤©ï¼‰")

            # æ„å»ºæœç´¢å…³é”®è¯
            precise_keywords = 'legal tech OR legaltech OR legal AI OR law technology OR æ³•å¾‹ç§‘æŠ€ OR æ³•å¾‹AI'
            logger.info(f"ğŸ”‘ æœç´¢å…³é”®è¯: {precise_keywords}")

            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            params = {
                'q': precise_keywords,
                'sortBy': 'relevance',
                'from': from_date,
                'to': today,
                'pageSize': 20,  # è·å–20æ¡ç”¨äºè¡¥å……
                'apiKey': self.config.news_api_key
            }

            # å‘é€APIè¯·æ±‚
            response = self.session.get(self.config.news_api_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()

            if data.get('status') == 'ok':
                newsapi_articles = data.get('articles', [])
                all_articles.extend(newsapi_articles)
                logger.info(f"âœ… NewsAPIè·å– {len(newsapi_articles)} æ¡æ–°é—»")

        except Exception as e:
            logger.warning(f"âš ï¸ NewsAPIè·å–å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨RSSæº")

        # ========== ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½å»é‡ï¼ˆURL + æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ” å¼€å§‹æ™ºèƒ½å»é‡...")
        logger.info("=" * 60)

        def is_similar_title(title1, title2):
            """åˆ¤æ–­ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦ç›¸ä¼¼ï¼ˆé¿å…é‡å¤æ–°é—»ï¼‰"""
            if not title1 or not title2:
                return False

            # æ¸…ç†æ ‡é¢˜
            t1 = title1.lower().strip()
            t2 = title2.lower().strip()

            # å®Œå…¨ç›¸åŒ
            if t1 == t2:
                return True

            # ä¸€ä¸ªæ ‡é¢˜æ˜¯å¦ä¸€ä¸ªçš„å­é›†ï¼ˆè¶…è¿‡80%ç›¸ä¼¼åº¦ï¼‰
            if len(t1) > 0 and len(t2) > 0:
                if t1 in t2 or t2 in t1:
                    # æ£€æŸ¥é•¿åº¦ç›¸ä¼¼åº¦
                    ratio = min(len(t1), len(t2)) / max(len(t1), len(t2))
                    if ratio >= 0.8:  # 80%ä»¥ä¸Šç›¸ä¼¼
                        return True

            return False

        # ç¬¬ä¸€å±‚å»é‡ï¼šURLå»é‡
        seen_urls = set()
        unique_by_url = []
        for article in all_articles:
            url = article.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_by_url.append(article)

        logger.info(f"âœ… URLå»é‡åå‰©ä½™ {len(unique_by_url)} æ¡æ–°é—»")

        # ç¬¬äºŒå±‚å»é‡ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦å»é‡
        unique_articles = []
        seen_titles = []

        for article in unique_by_url:
            title = article.get('title', '')
            is_duplicate = False

            # æ£€æŸ¥æ˜¯å¦ä¸å·²è§è¿‡çš„æ ‡é¢˜ç›¸ä¼¼
            for seen_title in seen_titles:
                if is_similar_title(title, seen_title):
                    is_duplicate = True
                    logger.debug(f"ğŸ”„ å‘ç°ç›¸ä¼¼æ ‡é¢˜ï¼Œå·²å»é‡: {title[:50]}...")
                    break

            if not is_duplicate:
                seen_titles.append(title)
                unique_articles.append(article)

        removed_count = len(unique_by_url) - len(unique_articles)
        if removed_count > 0:
            logger.info(f"âœ… æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ï¼šç§»é™¤ {removed_count} æ¡é‡å¤æ–°é—»")

        logger.info(f"âœ… æœ€ç»ˆå»é‡åå‰©ä½™ {len(unique_articles)} æ¡æ–°é—»")

        # ========== ç¬¬å››æ­¥ï¼šæ™ºèƒ½è¯„åˆ†æ’åºï¼ˆæ¥æºæƒé‡ + ç›¸å…³æ€§è¯„åˆ†ï¼‰==========
        logger.info("ğŸ¯ å¼€å§‹æ™ºèƒ½è¯„åˆ†æ’åº...")

        # å®šä¹‰æ¥æºæƒé‡
        source_weights = {
            'Artificial Lawyer': 3.0,      # æ³•å¾‹AIä¸“ä¸šåšå®¢
            'JDSupra Legal Tech': 3.0,     # ç§‘æŠ€æ³•å¾‹ä¸“ä¸šæ–‡ç« 
            'Above the Law': 2.0,          # æ³•å¾‹æ–°é—»
            'Law.com': 2.0,                # æ³•å¾‹æ–°é—»
            'LegalTechnology.News': 2.5,   # æ³•å¾‹ç§‘æŠ€ä¸“ä¸š
            'TechLaw': 2.0,                # æ³•å¾‹ç§‘æŠ€
            'Google News': 1.5,            # æ–°é—»èšåˆ
            'Business Insider': 1.0,       # é€šç”¨æ–°é—»
            'TechCrunch': 1.0,             # ç§‘æŠ€æ–°é—»
            'Forbes': 1.0,                 # å•†ä¸šæ–°é—»
        }

        # æ ¸å¿ƒå…³é”®è¯ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        primary_keywords = [
            'legal ai', 'legal artificial intelligence', 'æ³•å¾‹ai', 'æ³•å¾‹AI', 'æ³•å¾‹äººå·¥æ™ºèƒ½'
        ]
        secondary_keywords = [
            'legal tech', 'legal technology', 'legaltech', 'lawtech',
            'law tech', 'law technology', 'æ³•å¾‹ç§‘æŠ€'
        ]
        tertiary_keywords = [
            'legal automation', 'contract AI', 'e-discovery', 'document automation'
        ]

        def calculate_relevance_score(article):
            """è®¡ç®—å•æ¡æ–°é—»çš„ç›¸å…³æ€§å¾—åˆ†"""
            score = 0
            title = str(article.get('title') or '').lower()
            description = str(article.get('description') or '').lower()
            source = article.get('source', {}).get('name', '') or ''

            # 1. æ¥æºæƒé‡ï¼ˆ0-30åˆ†ï¼‰
            source_weight = source_weights.get(source, 1.0)
            score += source_weight * 10

            # 2. å…³é”®è¯åŒ¹é…å¾—åˆ†
            all_keywords = primary_keywords + secondary_keywords + tertiary_keywords

            # æ£€æŸ¥æ ‡é¢˜ä¸­çš„å…³é”®è¯åŒ¹é…
            for keyword in all_keywords:
                if keyword in title:
                    # æ ‡é¢˜å®Œå…¨åŒ¹é…
                    if keyword in primary_keywords:
                        score += 50  # æ ¸å¿ƒå…³é”®è¯åŒ¹é…
                    elif keyword in secondary_keywords:
                        score += 40  # æ¬¡è¦å…³é”®è¯åŒ¹é…
                    else:
                        score += 20  # å…¶ä»–å…³é”®è¯åŒ¹é…

                    # å…³é”®è¯åœ¨æ ‡é¢˜å¼€å¤´ï¼ˆå‰50ä¸ªå­—ç¬¦ï¼‰
                    if len(title) > 0 and keyword in title[:50]:
                        score += 10

            # æ£€æŸ¥æè¿°ä¸­çš„å…³é”®è¯åŒ¹é…
            desc_keyword_count = sum(1 for kw in all_keywords if kw in description)
            score += desc_keyword_count * 5  # æ¯ä¸ªå…³é”®è¯+5åˆ†

            # 3. æ—¶é—´æ–°é²œåº¦å¾—åˆ†ï¼ˆ0-20åˆ†ï¼‰
            pub_time = article.get('publishedAt', '')
            if pub_time:
                try:
                    from datetime import timezone, timedelta
                    dt = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)

                    now = datetime.now(timezone.utc)
                    time_diff = (now - dt).total_seconds() / 3600  # å°æ—¶

                    if time_diff <= 24:
                        score += 20  # 24å°æ—¶å†…
                    elif time_diff <= 48:
                        score += 10  # 48å°æ—¶å†…
                    elif time_diff <= 168:  # 7å¤©
                        score += 5
                except:
                    pass

            return score

        # ä¸ºæ¯æ¡æ–°é—»è®¡ç®—å¾—åˆ†å¹¶ä¿ç•™é€šè¿‡åŸºæœ¬ç­›é€‰çš„
        scored_articles = []
        for article in unique_articles:
            title = article.get('title', '').lower()
            description = article.get('description', '').lower()
            pub_time = article.get('publishedAt', '')

            # æ—¶é—´ç­›é€‰ï¼šåªä¿ç•™3å¤©å†…çš„æ–°é—»
            is_recent = True
            if pub_time:
                try:
                    from datetime import timezone, timedelta
                    dt = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)

                    now = datetime.now(timezone.utc)
                    time_diff = (now - dt).total_seconds() / 86400  # è½¬æ¢ä¸ºå¤©æ•°

                    if time_diff > 3:  # è¶…è¿‡3å¤©
                        is_recent = False
                except:
                    pass

            if not is_recent:
                continue  # è·³è¿‡è¶…è¿‡3å¤©çš„æ–°é—»

            # åŸºæœ¬ç­›é€‰ï¼šå¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå…³é”®è¯
            all_keywords = primary_keywords + secondary_keywords + tertiary_keywords
            contains_keyword = any(
                kw in title or kw in description
                for kw in all_keywords
            )

            if contains_keyword:
                score = calculate_relevance_score(article)
                article['_score'] = score
                scored_articles.append(article)

        # æŒ‰å¾—åˆ†æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
        scored_articles.sort(key=lambda x: x.get('_score', 0), reverse=True)

        logger.info(f"âœ… è¯„åˆ†åå‰©ä½™ {len(scored_articles)} æ¡ç²¾å‡†æ–°é—»")

        # æ˜¾ç¤ºå¾—åˆ†æœ€é«˜çš„å‰3æ¡æ–°é—»
        if scored_articles:
            logger.info("\nğŸ† å¾—åˆ†æœ€é«˜çš„æ–°é—»é¢„è§ˆ:")
            for i, article in enumerate(scored_articles[:3], 1):
                score = article.get('_score', 0)
                title = article.get('title', 'æ— æ ‡é¢˜')[:50]
                source = article.get('source', {}).get('name', 'æœªçŸ¥')
                logger.info(f"   {i}. [{score}åˆ†] {source}: {title}...")

        # ========== ç¬¬äº”æ­¥ï¼šå¤„ç†æ— æ–°é—»çš„æƒ…å†µ ==========
        if len(scored_articles) == 0:
            logger.warning("âš ï¸ ä»Šæ—¥æš‚æ— ç²¾å‡†çš„æ³•å¾‹ç§‘æŠ€/æ³•å¾‹AIç›¸å…³æ–°é—»")
            return [{
                'no_news_message': 'ä»Šæ—¥æš‚æ— ç²¾å‡†çš„æ³•å¾‹ç§‘æŠ€/æ³•å¾‹AIç›¸å…³æ–°é—»'
            }]

        # ========== ç¬¬å…­æ­¥ï¼šå–å‰10æ¡ï¼ˆæŒ‰ç»¼åˆå¾—åˆ†æ’åºï¼‰==========
        final_articles = scored_articles[:self.config.max_articles]
        logger.info(f"ğŸ¯ æœ€ç»ˆé€‰å– {len(final_articles)} æ¡æ–°é—»ï¼ˆæŒ‰ç»¼åˆå¾—åˆ†æ’åºï¼‰")

        # æ˜¾ç¤ºæ¥æºåˆ†å¸ƒ
        source_count = {}
        for article in final_articles:
            source = article.get('source', {}).get('name', 'æœªçŸ¥')
            source_count[source] = source_count.get(source, 0) + 1

        logger.info("\nğŸ“Š æ–°é—»æ¥æºåˆ†å¸ƒ:")
        for source, count in source_count.items():
            logger.info(f"   â€¢ {source}: {count} æ¡")

        return final_articles


# ====================== Claude AI å†…å®¹ç”Ÿæˆæ¨¡å— ======================
class NewsletterGenerator:
    """Newsletterç”Ÿæˆç±»ï¼šä½¿ç”¨Claude APIå°†æ–°é—»æ•´ç†æˆä¸­æ–‡æ ¼å¼"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–Newsletterç”Ÿæˆå™¨
        :param config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.session = requests.Session()

    def generate_newsletter(self, articles: List[Dict]) -> str:
        """
        ä½¿ç”¨Claude APIç”Ÿæˆä¸­æ–‡Newsletter
        :param articles: æ–°é—»åˆ—è¡¨
        :return: æ ¼å¼åŒ–åçš„ä¸­æ–‡Newsletteræ–‡æœ¬
        """
        # ========== æ£€æŸ¥æ˜¯å¦æœ‰"æ— æ–°é—»"çš„æ ‡è®° ==========
        if not articles:
            logger.warning("âš ï¸ æ²¡æœ‰æ–°é—»å¯ä¾›æ•´ç†")
            return "ä»Šæ—¥æš‚æ— æ³•å¾‹ç§‘æŠ€ç›¸å…³æ–°é—»"

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ— ç²¾å‡†æ–°é—»çš„ç‰¹æ®Šæ ‡è®°
        if len(articles) == 1 and 'no_news_message' in articles[0]:
            logger.info("ğŸ“­ ä»Šæ—¥æš‚æ— ç²¾å‡†æ–°é—»ï¼Œä½¿ç”¨æç¤ºä¿¡æ¯")
            date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
            return f"""ğŸ“° æ³•å¾‹ç§‘æŠ€æ—¥æŠ¥ - {date_str}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“­ {articles[0]['no_news_message']}

ğŸ’¡ æç¤ºï¼šä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ ‡å‡†çš„æ³•å¾‹ç§‘æŠ€/æ³•å¾‹AIç›¸å…³æ–°é—»ã€‚è¿™å¯èƒ½æ˜¯ç”±äºï¼š
  â€¢ å½“å¤©ç›¸å…³æ–°é—»å‘å¸ƒè¾ƒå°‘
  â€¢ æ–°é—»æ¥æºæœªè¦†ç›–æ‰€æœ‰æ¸ é“
  â€¢ å…³é”®è¯åŒ¹é…éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¤– ç”±æ³•å¾‹ç§‘æŠ€æ–°é—»Botè‡ªåŠ¨æ¨é€"""

        # å¦‚æœæ²¡æœ‰Claude APIå¯†é’¥ï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        if not self.config.claude_api_key:
            logger.info("ğŸ’¡ æœªé…ç½®Claude APIï¼Œä½¿ç”¨ç®€å•æ ¼å¼")
            return self._fallback_newsletter(articles)

        logger.info("ğŸ¤– å¼€å§‹ä½¿ç”¨Claude APIç”ŸæˆNewsletter...")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        if not self.config.enable_fallback:
            logger.info("ğŸ“Œ å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨ï¼Œä»…ä½¿ç”¨Claude API")

        try:
            # æ„å»ºå‘é€ç»™Claudeçš„æ–°é—»æ‘˜è¦
            news_summary = self._prepare_news_summary(articles)

            # æ„å»ºClaude APIè¯·æ±‚
            headers = {
                'x-api-key': self.config.claude_api_key,
                'anthropic-version': '2023-06-01',
                'content-type': 'application/json'
            }

            # Claude APIè¯·æ±‚ä½“
            payload = {
                'model': 'claude-3-5-haiku-20241022',  # ä½¿ç”¨Haikuæ¨¡å‹ï¼Œæˆæœ¬ä½
                'max_tokens': self.config.claude_max_tokens,  # æœ€å¤§è¿”å›tokenæ•°
                'messages': [{
                    'role': 'user',
                    'content': self._build_prompt(news_summary)
                }]
            }

            # å‘é€è¯·æ±‚åˆ°Claude API
            response = self.session.post(
                self.config.claude_api_url,
                headers=headers,
                json=payload,
                timeout=30  # 30ç§’è¶…æ—¶
            )

            # æ£€æŸ¥å“åº”çŠ¶æ€
            response.raise_for_status()

            # è§£æå“åº”
            result = response.json()
            newsletter_content = result['content'][0]['text']

            logger.info("âœ… Newsletterç”ŸæˆæˆåŠŸï¼ˆä½¿ç”¨Claude APIï¼‰")
            return newsletter_content

        except requests.exceptions.HTTPError as e:
            logger.error(f"âŒ Claude APIè¯·æ±‚å¤±è´¥: {e}")
            if e.response.status_code == 401:
                logger.error("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥CLAUDE_API_KEYæ˜¯å¦æ­£ç¡®")
            elif e.response.status_code == 429:
                logger.error("ğŸ’¡ æç¤ºï¼šè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•")
            elif e.response.status_code == 400:
                logger.error("ğŸ’¡ æç¤ºï¼šè¯·æ±‚å‚æ•°é”™è¯¯æˆ–APIä½™é¢ä¸è¶³")

            # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if self.config.enable_fallback:
                logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆ...")
                return self._fallback_newsletter(articles)
            else:
                logger.error("âŒ å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨ï¼Œæ— æ³•ç”ŸæˆNewsletter")
                return "æŠ±æ­‰ï¼ŒNewsletterç”Ÿæˆå¤±è´¥ï¼Œä¸”å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨"

        except requests.exceptions.Timeout:
            logger.error("âŒ Claude APIè¯·æ±‚è¶…æ—¶")
            if self.config.enable_fallback:
                logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆ...")
                return self._fallback_newsletter(articles)
            else:
                return "æŠ±æ­‰ï¼ŒAPIè¯·æ±‚è¶…æ—¶ï¼Œä¸”å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨"

        except Exception as e:
            logger.error(f"âŒ Newsletterç”Ÿæˆå¤±è´¥: {e}")
            if self.config.enable_fallback:
                logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆ...")
                return self._fallback_newsletter(articles)
            else:
                return "æŠ±æ­‰ï¼ŒNewsletterç”Ÿæˆå¤±è´¥ï¼Œä¸”å¤‡ç”¨æ–¹æ¡ˆå·²ç¦ç”¨"

    def _prepare_news_summary(self, articles: List[Dict]) -> str:
        """
        å°†æ–°é—»åˆ—è¡¨æ ¼å¼åŒ–ä¸ºæ–‡æœ¬æ‘˜è¦
        :param articles: æ–°é—»åˆ—è¡¨
        :return: æ ¼å¼åŒ–çš„æ–‡æœ¬æ‘˜è¦
        """
        summary_lines = []
        for i, article in enumerate(articles, 1):
            title = article.get('title', 'æ— æ ‡é¢˜')
            description = article.get('description', 'æ— æè¿°')
            url = article.get('url', '')
            source = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
            published_at = article.get('publishedAt', '')

            summary_lines.append(
                f"{i}. æ ‡é¢˜: {title}\n"
                f"   æ¥æº: {source}\n"
                f"   æè¿°: {description}\n"
                f"   é“¾æ¥: {url}\n"
            )

        return '\n'.join(summary_lines)

    def _build_prompt(self, news_summary: str) -> str:
        """
        æ„å»ºå‘é€ç»™Claudeçš„æç¤ºè¯
        :param news_summary: æ–°é—»æ‘˜è¦
        :return: å®Œæ•´çš„æç¤ºè¯
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹ç§‘æŠ€æ–°é—»ç¼–è¾‘ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–°é—»æ•´ç†æˆä¸€ä»½ç®€æ˜æ‰¼è¦çš„ä¸­æ–‡Newsletterã€‚

è¦æ±‚ï¼š
1. æ ‡é¢˜ï¼šä½¿ç”¨å¸å¼•äººçš„æ ‡é¢˜ï¼ŒåŒ…å«æ—¥æœŸ
2. æ ¼å¼ï¼šæ¯æ¡æ–°é—»åŒ…å«ã€æ ‡é¢˜ã€‘ã€ã€ç®€çŸ­æ‘˜è¦ã€‘ã€ã€æ¥æºã€‘ã€ã€é“¾æ¥ã€‘
3. è¯­è¨€ï¼šå…¨éƒ¨ä½¿ç”¨ä¸­æ–‡
4. é£æ ¼ï¼šç®€æ´ä¸“ä¸šï¼Œé€‚åˆå¿«é€Ÿé˜…è¯»
5. æœ€å¤šæ•´ç†8æ¡æœ€é‡è¦æ–°é—»
6. ä½¿ç”¨emojiè®©ç‰ˆé¢æ›´ç”ŸåŠ¨

ä»¥ä¸‹æ˜¯ä»Šå¤©çš„æ–°é—»ï¼š

{news_summary}

è¯·ç›´æ¥è¾“å‡ºNewsletterå†…å®¹ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šã€‚"""

        return prompt

    def _clean_html(self, html_text: str) -> str:
        """
        æ¸…ç†HTMLæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬
        :param html_text: åŒ…å«HTMLæ ‡ç­¾çš„æ–‡æœ¬
        :return: æ¸…ç†åçš„çº¯æ–‡æœ¬
        """
        if not html_text:
            return html_text

        # ç§»é™¤HTMLæ ‡ç­¾
        # ç§»é™¤<script>æ ‡ç­¾åŠå…¶å†…å®¹
        clean_text = re.sub(r'<script.*?>.*?</script>', '', html_text, flags=re.DOTALL | re.IGNORECASE)
        # ç§»é™¤<style>æ ‡ç­¾åŠå…¶å†…å®¹
        clean_text = re.sub(r'<style.*?>.*?</style>', '', clean_text, flags=re.DOTALL | re.IGNORECASE)
        # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾
        clean_text = re.sub(r'<[^>]+>', '', clean_text)
        # ç§»é™¤HTMLå®ä½“ï¼ˆå¦‚&nbsp;, &lt;, &gt;ç­‰ï¼‰
        clean_text = re.sub(r'&nbsp;', ' ', clean_text)
        clean_text = re.sub(r'&lt;', '<', clean_text)
        clean_text = re.sub(r'&gt;', '>', clean_text)
        clean_text = re.sub(r'&amp;', '&', clean_text)
        clean_text = re.sub(r'&quot;', '"', clean_text)
        clean_text = re.sub(r'&#39;', "'", clean_text)
        clean_text = re.sub(r'&apos;', "'", clean_text)
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text)
        # å»é™¤é¦–å°¾ç©ºæ ¼
        clean_text = clean_text.strip()

        return clean_text

    def _translate_text(self, text: str, max_length: int = 5000) -> str:
        """
        ä½¿ç”¨å…è´¹çš„Googleç¿»è¯‘ç¿»è¯‘æ–‡æœ¬
        :param text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        :param max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼ˆGoogleç¿»è¯‘é™åˆ¶ï¼‰
        :return: ç¿»è¯‘åçš„æ–‡æœ¬
        """
        if not text or not text.strip():
            return text

        try:
            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œæˆªæ–­ç¿»è¯‘
            if len(text) > max_length:
                text = text[:max_length] + "..."

            translator = GoogleTranslator(source='auto', target='zh-CN')
            translated = translator.translate(text)
            return translated
        except Exception as e:
            logger.warning(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}ï¼Œä¿ç•™åŸæ–‡")
            return text

    def _fallback_newsletter(self, articles: List[Dict]) -> str:
        """
        å¤‡ç”¨æ–¹æ¡ˆï¼šå½“Claude APIè°ƒç”¨å¤±è´¥æ—¶ï¼Œä½¿ç”¨ç®€å•çš„æ ¼å¼åŒ–
        ç°åœ¨åŒ…å«å…è´¹ç¿»è¯‘åŠŸèƒ½
        :param articles: æ–°é—»åˆ—è¡¨
        :return: ç®€å•æ ¼å¼åŒ–çš„æ–°é—»æ–‡æœ¬
        """
        logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆç”ŸæˆNewsletterï¼ˆåŒ…å«å…è´¹ç¿»è¯‘ï¼‰")

        date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        lines = [
            f"ğŸ“° æ³•å¾‹ç§‘æŠ€æ—¥æŠ¥ - {date_str}",
            "",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            ""
        ]

        # æœ€å¤šæ˜¾ç¤ºé…ç½®çš„æ–°é—»æ•°é‡ï¼ˆé»˜è®¤15æ¡ï¼‰
        for i, article in enumerate(articles[:self.config.max_articles], 1):
            title = article.get('title', 'æ— æ ‡é¢˜')
            description = article.get('description', '')
            url = article.get('url', '')
            source = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
            published_at = article.get('publishedAt', '')

            # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
            publish_time = ''
            if published_at:
                try:
                    # è§£æISOæ ¼å¼æ—¶é—´å¹¶è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
                    dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´æ˜¾ç¤º
                    publish_time = dt.strftime('%Y-%m-%d %H:%M')
                except:
                    publish_time = published_at

            # ç¿»è¯‘æ ‡é¢˜å’Œæè¿°ï¼ˆå…ˆæ¸…ç†HTMLæ ‡ç­¾ï¼‰
            try:
                # å…ˆæ¸…ç†HTMLæ ‡ç­¾
                title_clean = self._clean_html(title)
                description_clean = self._clean_html(description) if description else ""

                # è®¡ç®—æ ‡é¢˜å’Œæ‘˜è¦çš„ç›¸ä¼¼åº¦
                should_show_description = False
                if description_clean:
                    # æ ‡å‡†åŒ–ä¸¤ä¸ªå­—ç¬¦ä¸²ï¼ˆå»é™¤å¤§å°å†™ã€æ ‡ç‚¹ã€ç©ºæ ¼ï¼‰
                    title_normalized = title_clean.lower()
                    desc_normalized = description_clean.lower()

                    # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
                    translator = str.maketrans('', '', string.punctuation + ' ')
                    title_normalized = title_normalized.translate(translator)
                    desc_normalized = desc_normalized.translate(translator)

                    # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
                    similarity = SequenceMatcher(None, title_normalized, desc_normalized).ratio()

                    # å¦‚æœç›¸ä¼¼åº¦ä½äº95%ï¼Œæ‰æ˜¾ç¤ºæ‘˜è¦
                    if similarity < 0.95:
                        should_show_description = True
                    else:
                        logger.debug(f"ğŸš« æ–°é—»{i}: æ ‡é¢˜æ‘˜è¦ç›¸ä¼¼åº¦{similarity:.1%}ï¼Œä¸æ˜¾ç¤ºæ‘˜è¦")

                # å†ç¿»è¯‘
                title_translated = self._translate_text(title_clean)
                description_translated = self._translate_text(description_clean) if should_show_description else ""

                lines.append(f"ğŸ“Œ æ–°é—» {i}")
                lines.append(f"æ ‡é¢˜: {title_translated}")
                if description_translated:
                    lines.append(f"æ‘˜è¦: {description_translated}")
                lines.append(f"æ¥æº: {source}")
                if publish_time:
                    lines.append(f"å‘å¸ƒæ—¶é—´: {publish_time}")
                lines.append(f"é“¾æ¥: {url}")
                lines.append("")
            except Exception as e:
                logger.warning(f"âš ï¸ ç¿»è¯‘æ–°é—» {i} å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ–‡")
                lines.append(f"ğŸ“Œ æ–°é—» {i}")
                lines.append(f"æ ‡é¢˜: {title}")
                if description and description.lower() != title.lower():
                    lines.append(f"æ‘˜è¦: {description}")
                lines.append(f"æ¥æº: {source}")
                if publish_time:
                    lines.append(f"å‘å¸ƒæ—¶é—´: {publish_time}")
                lines.append(f"é“¾æ¥: {url}")
                lines.append("")

        lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        lines.append("")
        lines.append("ğŸ¤– ç”±æ³•å¾‹ç§‘æŠ€æ–°é—»Botè‡ªåŠ¨æ¨é€ï¼ˆä½¿ç”¨å…è´¹ç¿»è¯‘ï¼‰")

        return '\n'.join(lines)


# ====================== é£ä¹¦æ¨é€æ¨¡å— ======================
class FeishuNotifier:
    """é£ä¹¦é€šçŸ¥ç±»ï¼šé€šè¿‡Webhookå‘é€æ¶ˆæ¯åˆ°é£ä¹¦ç¾¤"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–é£ä¹¦é€šçŸ¥å™¨
        :param config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.session = requests.Session()

    def send_newsletter(self, newsletter_content: str):
        """
        å‘é€Newsletteråˆ°é£ä¹¦ç¾¤
        :param newsletter_content: Newsletterå†…å®¹
        """
        logger.info("ğŸ“¤ å¼€å§‹å‘é€é£ä¹¦é€šçŸ¥...")

        try:
            # æ„å»ºé£ä¹¦æ¶ˆæ¯æ ¼å¼
            message = {
                "msg_type": "text",
                "content": {
                    "text": newsletter_content
                }
            }

            # å‘é€POSTè¯·æ±‚åˆ°é£ä¹¦Webhook
            response = self.session.post(
                self.config.feishu_webhook,
                json=message,
                timeout=10
            )

            # æ£€æŸ¥å“åº”çŠ¶æ€
            response.raise_for_status()

            result = response.json()

            # æ£€æŸ¥é£ä¹¦APIè¿”å›ç 
            if result.get('code') == 0:
                logger.info("âœ… é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                logger.error(f"âŒ é£ä¹¦APIè¿”å›é”™è¯¯: {result}")

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e}")


# ====================== Botä¸»æ§åˆ¶å™¨ ======================
class LegalTechNewsBot:
    """æ³•å¾‹ç§‘æŠ€æ–°é—»Botä¸»æ§åˆ¶å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–Bot"""
        logger.info("=" * 60)
        logger.info("ğŸš€ æ³•å¾‹ç§‘æŠ€æ–°é—»Botå¯åŠ¨")
        logger.info("=" * 60)

        # åˆå§‹åŒ–é…ç½®
        self.config = Config()

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.news_fetcher = NewsFetcher(self.config)
        self.newsletter_generator = NewsletterGenerator(self.config)
        self.feishu_notifier = FeishuNotifier(self.config)

    def run_daily_task(self):
        """
        æ‰§è¡Œæ¯æ—¥ä»»åŠ¡ï¼šæŠ“å–æ–°é—» -> ç”ŸæˆNewsletter -> æ¨é€åˆ°é£ä¹¦
        """
        logger.info("\n" + "=" * 60)
        logger.info(f"â° å¼€å§‹æ‰§è¡Œæ¯æ—¥ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60 + "\n")

        try:
            # æ­¥éª¤1: è·å–æ–°é—»
            articles = self.news_fetcher.fetch_legal_tech_news()

            if not articles:
                logger.warning("âš ï¸ æœªè·å–åˆ°æ–°é—»ï¼Œä»»åŠ¡ç»“æŸ")
                return

            # æ­¥éª¤2: ç”ŸæˆNewsletter
            newsletter = self.newsletter_generator.generate_newsletter(articles)

            # æ­¥éª¤3: å‘é€åˆ°é£ä¹¦
            self.feishu_notifier.send_newsletter(newsletter)

            logger.info("\n" + "=" * 60)
            logger.info("âœ… ä»Šæ—¥ä»»åŠ¡å®Œæˆ")
            logger.info("=" * 60 + "\n")

        except Exception as e:
            logger.error(f"\nâŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            logger.error("=" * 60 + "\n")

    def start(self):
        """
        å¯åŠ¨Botï¼šè®¾ç½®å®šæ—¶ä»»åŠ¡ï¼Œæ¯å¤©ä¸­åˆ12ç‚¹æ‰§è¡Œ
        """
        # è®¾ç½®æ¯å¤©12:00æ‰§è¡Œä»»åŠ¡
        schedule.every().day.at("12:00").do(self.run_daily_task)

        logger.info("ğŸ“… Botå·²å¯åŠ¨ï¼Œå°†åœ¨æ¯å¤©ä¸­åˆ12:00æ¨é€æ–°é—»")
        logger.info("ğŸ’¡ æç¤ºï¼šæŒ‰Ctrl+Cå¯ä»¥åœæ­¢Bot\n")

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•ï¼‰
        # self.run_daily_task()

        # æŒç»­è¿è¡Œ
        try:
            while True:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡
                schedule.run_pending()
                # æ¯éš”60ç§’æ£€æŸ¥ä¸€æ¬¡
                time.sleep(60)

        except KeyboardInterrupt:
            logger.info("\nğŸ‘‹ Botå·²åœæ­¢")

    def run_once(self):
        """
        å•æ¬¡è¿è¡Œæ¨¡å¼ï¼šç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        logger.info("ğŸ§ª å•æ¬¡è¿è¡Œæ¨¡å¼")
        self.run_daily_task()


# ====================== ä¸»ç¨‹åºå…¥å£ ======================
def main():
    """
    ä¸»å‡½æ•°ï¼šç¨‹åºå…¥å£
    æ”¯æŒä¸¤ç§è¿è¡Œæ¨¡å¼ï¼š
    1. testæ¨¡å¼ï¼šç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    2. productionæ¨¡å¼ï¼šå®šæ—¶è¿è¡Œï¼Œæ¯å¤©12ç‚¹æ‰§è¡Œï¼ˆç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

    æ¨¡å¼åˆ‡æ¢ï¼šåœ¨.envæ–‡ä»¶ä¸­è®¾ç½® RUN_MODE=test æˆ– RUN_MODE=production
    """
    try:
        # åˆ›å»ºBotå®ä¾‹
        bot = LegalTechNewsBot()

        # æ ¹æ®é…ç½®é€‰æ‹©è¿è¡Œæ¨¡å¼
        if bot.config.run_mode == 'test':
            logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚éœ€åˆ‡æ¢åˆ°å®šæ—¶æ¨¡å¼ï¼Œè¯·åœ¨.envä¸­è®¾ç½® RUN_MODE=production")
            bot.run_once()
        else:
            logger.info("ğŸš€ ç”Ÿäº§æ¨¡å¼ï¼šå®šæ—¶è¿è¡Œä¸­")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚éœ€åˆ‡æ¢åˆ°æµ‹è¯•æ¨¡å¼ï¼Œè¯·åœ¨.envä¸­è®¾ç½® RUN_MODE=test")
            bot.start()

    except ValueError as e:
        logger.error(f"âŒ é…ç½®é”™è¯¯: {e}")
        logger.error("è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„é…ç½®é¡¹æ˜¯å¦æ­£ç¡®")

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")


# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œåˆ™æ‰§è¡Œmainå‡½æ•°
if __name__ == '__main__':
    main()
